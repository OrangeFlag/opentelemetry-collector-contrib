// Code generated by mdatagen. DO NOT EDIT.

package metadata

import (
	"fmt"
	"time"

	"go.opentelemetry.io/collector/model/pdata"
)

// MetricSettings provides common settings for a particular metric.
type MetricSettings struct {
	Enabled bool `mapstructure:"enabled"`
}

// MetricsSettings provides settings for elasticsearchreceiver metrics.
type MetricsSettings struct {
	ElasticsearchClusterDataNodes            MetricSettings `mapstructure:"elasticsearch.cluster.data_nodes"`
	ElasticsearchClusterHealth               MetricSettings `mapstructure:"elasticsearch.cluster.health"`
	ElasticsearchClusterNodes                MetricSettings `mapstructure:"elasticsearch.cluster.nodes"`
	ElasticsearchClusterShards               MetricSettings `mapstructure:"elasticsearch.cluster.shards"`
	ElasticsearchNodeCacheEvictions          MetricSettings `mapstructure:"elasticsearch.node.cache.evictions"`
	ElasticsearchNodeCacheMemoryUsage        MetricSettings `mapstructure:"elasticsearch.node.cache.memory.usage"`
	ElasticsearchNodeClusterConnections      MetricSettings `mapstructure:"elasticsearch.node.cluster.connections"`
	ElasticsearchNodeClusterIo               MetricSettings `mapstructure:"elasticsearch.node.cluster.io"`
	ElasticsearchNodeDocuments               MetricSettings `mapstructure:"elasticsearch.node.documents"`
	ElasticsearchNodeFsDiskAvailable         MetricSettings `mapstructure:"elasticsearch.node.fs.disk.available"`
	ElasticsearchNodeHTTPConnections         MetricSettings `mapstructure:"elasticsearch.node.http.connections"`
	ElasticsearchNodeOpenFiles               MetricSettings `mapstructure:"elasticsearch.node.open_files"`
	ElasticsearchNodeOperationsCompleted     MetricSettings `mapstructure:"elasticsearch.node.operations.completed"`
	ElasticsearchNodeOperationsTime          MetricSettings `mapstructure:"elasticsearch.node.operations.time"`
	ElasticsearchNodeShardsSize              MetricSettings `mapstructure:"elasticsearch.node.shards.size"`
	ElasticsearchNodeThreadPoolTasksFinished MetricSettings `mapstructure:"elasticsearch.node.thread_pool.tasks.finished"`
	ElasticsearchNodeThreadPoolTasksQueued   MetricSettings `mapstructure:"elasticsearch.node.thread_pool.tasks.queued"`
	ElasticsearchNodeThreadPoolThreads       MetricSettings `mapstructure:"elasticsearch.node.thread_pool.threads"`
	JvmClassesLoaded                         MetricSettings `mapstructure:"jvm.classes.loaded"`
	JvmGcCollectionsCount                    MetricSettings `mapstructure:"jvm.gc.collections.count"`
	JvmGcCollectionsElapsed                  MetricSettings `mapstructure:"jvm.gc.collections.elapsed"`
	JvmMemoryHeapCommitted                   MetricSettings `mapstructure:"jvm.memory.heap.committed"`
	JvmMemoryHeapMax                         MetricSettings `mapstructure:"jvm.memory.heap.max"`
	JvmMemoryHeapUsed                        MetricSettings `mapstructure:"jvm.memory.heap.used"`
	JvmMemoryNonheapCommitted                MetricSettings `mapstructure:"jvm.memory.nonheap.committed"`
	JvmMemoryNonheapUsed                     MetricSettings `mapstructure:"jvm.memory.nonheap.used"`
	JvmMemoryPoolMax                         MetricSettings `mapstructure:"jvm.memory.pool.max"`
	JvmMemoryPoolUsed                        MetricSettings `mapstructure:"jvm.memory.pool.used"`
	JvmThreadsCount                          MetricSettings `mapstructure:"jvm.threads.count"`
}

func DefaultMetricsSettings() MetricsSettings {
	return MetricsSettings{
		ElasticsearchClusterDataNodes: MetricSettings{
			Enabled: true,
		},
		ElasticsearchClusterHealth: MetricSettings{
			Enabled: true,
		},
		ElasticsearchClusterNodes: MetricSettings{
			Enabled: true,
		},
		ElasticsearchClusterShards: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeCacheEvictions: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeCacheMemoryUsage: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeClusterConnections: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeClusterIo: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeDocuments: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeFsDiskAvailable: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeHTTPConnections: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeOpenFiles: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeOperationsCompleted: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeOperationsTime: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeShardsSize: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeThreadPoolTasksFinished: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeThreadPoolTasksQueued: MetricSettings{
			Enabled: true,
		},
		ElasticsearchNodeThreadPoolThreads: MetricSettings{
			Enabled: true,
		},
		JvmClassesLoaded: MetricSettings{
			Enabled: true,
		},
		JvmGcCollectionsCount: MetricSettings{
			Enabled: true,
		},
		JvmGcCollectionsElapsed: MetricSettings{
			Enabled: true,
		},
		JvmMemoryHeapCommitted: MetricSettings{
			Enabled: true,
		},
		JvmMemoryHeapMax: MetricSettings{
			Enabled: true,
		},
		JvmMemoryHeapUsed: MetricSettings{
			Enabled: true,
		},
		JvmMemoryNonheapCommitted: MetricSettings{
			Enabled: true,
		},
		JvmMemoryNonheapUsed: MetricSettings{
			Enabled: true,
		},
		JvmMemoryPoolMax: MetricSettings{
			Enabled: true,
		},
		JvmMemoryPoolUsed: MetricSettings{
			Enabled: true,
		},
		JvmThreadsCount: MetricSettings{
			Enabled: true,
		},
	}
}

type MetricIntf interface {
	GetName() string
	GetDescription() string
	GetUnit() string
	GetMetricType() MetricDataTypeMetadata
}

type MetricDataTypeMetadata struct {
	Sum   *Sum   `yaml:"sum"`
	Gauge *Gauge `yaml:"gauge"`
}

type Gauge struct {
	ValueType string
}

type Sum struct {
	Aggregation pdata.MetricAggregationTemporality
	Monotonic   bool
	ValueType   string
}

type metricElasticsearchClusterDataNodes struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.cluster.data_nodes metric with initial data.
func (m *metricElasticsearchClusterDataNodes) init() {
	m.data.SetName("elasticsearch.cluster.data_nodes")
	m.data.SetDescription("The number of data nodes in the cluster.")
	m.data.SetUnit("{nodes}")
	m.data.SetDataType(pdata.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pdata.MetricAggregationTemporalityCumulative)
}

type MetricMetadataElasticsearchClusterDataNodes struct{}

func (m MetricMetadataElasticsearchClusterDataNodes) GetName() string {
	return "elasticsearch.cluster.data_nodes"
}

func (m MetricMetadataElasticsearchClusterDataNodes) GetDescription() string {
	return "The number of data nodes in the cluster."
}

func (m MetricMetadataElasticsearchClusterDataNodes) GetUnit() string {
	return "{nodes}"
}

func (m MetricMetadataElasticsearchClusterDataNodes) GetValueType() string {
	return "int64"
}

func (m MetricMetadataElasticsearchClusterDataNodes) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Sum: &Sum{
			Aggregation: pdata.MetricAggregationTemporalityCumulative,
			Monotonic:   false,
			ValueType:   "Int",
		},
	}
}

func (m *metricElasticsearchClusterDataNodes) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchClusterDataNodes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchClusterDataNodes) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchClusterDataNodes(settings MetricSettings) metricElasticsearchClusterDataNodes {
	m := metricElasticsearchClusterDataNodes{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchClusterHealth struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.cluster.health metric with initial data.
func (m *metricElasticsearchClusterHealth) init() {
	m.data.SetName("elasticsearch.cluster.health")
	m.data.SetDescription("The health status of the cluster.")
	m.data.SetUnit("{status}")
	m.data.SetDataType(pdata.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pdata.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

type MetricMetadataElasticsearchClusterHealth struct{}

func (m MetricMetadataElasticsearchClusterHealth) GetName() string {
	return "elasticsearch.cluster.health"
}

func (m MetricMetadataElasticsearchClusterHealth) GetDescription() string {
	return "The health status of the cluster."
}

func (m MetricMetadataElasticsearchClusterHealth) GetUnit() string {
	return "{status}"
}

func (m MetricMetadataElasticsearchClusterHealth) GetValueType() string {
	return "int64"
}

func (m MetricMetadataElasticsearchClusterHealth) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Sum: &Sum{
			Aggregation: pdata.MetricAggregationTemporalityCumulative,
			Monotonic:   false,
			ValueType:   "Int",
		},
	}
}

func (m *metricElasticsearchClusterHealth) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64, healthStatusAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.HealthStatus, pdata.NewValueString(healthStatusAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchClusterHealth) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchClusterHealth) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchClusterHealth(settings MetricSettings) metricElasticsearchClusterHealth {
	m := metricElasticsearchClusterHealth{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchClusterNodes struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.cluster.nodes metric with initial data.
func (m *metricElasticsearchClusterNodes) init() {
	m.data.SetName("elasticsearch.cluster.nodes")
	m.data.SetDescription("The total number of nodes in the cluster.")
	m.data.SetUnit("{nodes}")
	m.data.SetDataType(pdata.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pdata.MetricAggregationTemporalityCumulative)
}

type MetricMetadataElasticsearchClusterNodes struct{}

func (m MetricMetadataElasticsearchClusterNodes) GetName() string {
	return "elasticsearch.cluster.nodes"
}

func (m MetricMetadataElasticsearchClusterNodes) GetDescription() string {
	return "The total number of nodes in the cluster."
}

func (m MetricMetadataElasticsearchClusterNodes) GetUnit() string {
	return "{nodes}"
}

func (m MetricMetadataElasticsearchClusterNodes) GetValueType() string {
	return "int64"
}

func (m MetricMetadataElasticsearchClusterNodes) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Sum: &Sum{
			Aggregation: pdata.MetricAggregationTemporalityCumulative,
			Monotonic:   false,
			ValueType:   "Int",
		},
	}
}

func (m *metricElasticsearchClusterNodes) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchClusterNodes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchClusterNodes) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchClusterNodes(settings MetricSettings) metricElasticsearchClusterNodes {
	m := metricElasticsearchClusterNodes{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchClusterShards struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.cluster.shards metric with initial data.
func (m *metricElasticsearchClusterShards) init() {
	m.data.SetName("elasticsearch.cluster.shards")
	m.data.SetDescription("The number of shards in the cluster.")
	m.data.SetUnit("{shards}")
	m.data.SetDataType(pdata.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pdata.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

type MetricMetadataElasticsearchClusterShards struct{}

func (m MetricMetadataElasticsearchClusterShards) GetName() string {
	return "elasticsearch.cluster.shards"
}

func (m MetricMetadataElasticsearchClusterShards) GetDescription() string {
	return "The number of shards in the cluster."
}

func (m MetricMetadataElasticsearchClusterShards) GetUnit() string {
	return "{shards}"
}

func (m MetricMetadataElasticsearchClusterShards) GetValueType() string {
	return "int64"
}

func (m MetricMetadataElasticsearchClusterShards) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Sum: &Sum{
			Aggregation: pdata.MetricAggregationTemporalityCumulative,
			Monotonic:   false,
			ValueType:   "Int",
		},
	}
}

func (m *metricElasticsearchClusterShards) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64, shardStateAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.ShardState, pdata.NewValueString(shardStateAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchClusterShards) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchClusterShards) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchClusterShards(settings MetricSettings) metricElasticsearchClusterShards {
	m := metricElasticsearchClusterShards{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeCacheEvictions struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.cache.evictions metric with initial data.
func (m *metricElasticsearchNodeCacheEvictions) init() {
	m.data.SetName("elasticsearch.node.cache.evictions")
	m.data.SetDescription("The number of evictions from the cache.")
	m.data.SetUnit("{evictions}")
	m.data.SetDataType(pdata.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pdata.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

type MetricMetadataElasticsearchNodeCacheEvictions struct{}

func (m MetricMetadataElasticsearchNodeCacheEvictions) GetName() string {
	return "elasticsearch.node.cache.evictions"
}

func (m MetricMetadataElasticsearchNodeCacheEvictions) GetDescription() string {
	return "The number of evictions from the cache."
}

func (m MetricMetadataElasticsearchNodeCacheEvictions) GetUnit() string {
	return "{evictions}"
}

func (m MetricMetadataElasticsearchNodeCacheEvictions) GetValueType() string {
	return "int64"
}

func (m MetricMetadataElasticsearchNodeCacheEvictions) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Sum: &Sum{
			Aggregation: pdata.MetricAggregationTemporalityCumulative,
			Monotonic:   true,
			ValueType:   "Int",
		},
	}
}

func (m *metricElasticsearchNodeCacheEvictions) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64, cacheNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.CacheName, pdata.NewValueString(cacheNameAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeCacheEvictions) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeCacheEvictions) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeCacheEvictions(settings MetricSettings) metricElasticsearchNodeCacheEvictions {
	m := metricElasticsearchNodeCacheEvictions{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeCacheMemoryUsage struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.cache.memory.usage metric with initial data.
func (m *metricElasticsearchNodeCacheMemoryUsage) init() {
	m.data.SetName("elasticsearch.node.cache.memory.usage")
	m.data.SetDescription("The size in bytes of the cache.")
	m.data.SetUnit("By")
	m.data.SetDataType(pdata.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pdata.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

type MetricMetadataElasticsearchNodeCacheMemoryUsage struct{}

func (m MetricMetadataElasticsearchNodeCacheMemoryUsage) GetName() string {
	return "elasticsearch.node.cache.memory.usage"
}

func (m MetricMetadataElasticsearchNodeCacheMemoryUsage) GetDescription() string {
	return "The size in bytes of the cache."
}

func (m MetricMetadataElasticsearchNodeCacheMemoryUsage) GetUnit() string {
	return "By"
}

func (m MetricMetadataElasticsearchNodeCacheMemoryUsage) GetValueType() string {
	return "int64"
}

func (m MetricMetadataElasticsearchNodeCacheMemoryUsage) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Sum: &Sum{
			Aggregation: pdata.MetricAggregationTemporalityCumulative,
			Monotonic:   false,
			ValueType:   "Int",
		},
	}
}

func (m *metricElasticsearchNodeCacheMemoryUsage) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64, cacheNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.CacheName, pdata.NewValueString(cacheNameAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeCacheMemoryUsage) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeCacheMemoryUsage) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeCacheMemoryUsage(settings MetricSettings) metricElasticsearchNodeCacheMemoryUsage {
	m := metricElasticsearchNodeCacheMemoryUsage{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeClusterConnections struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.cluster.connections metric with initial data.
func (m *metricElasticsearchNodeClusterConnections) init() {
	m.data.SetName("elasticsearch.node.cluster.connections")
	m.data.SetDescription("The number of open tcp connections for internal cluster communication.")
	m.data.SetUnit("{connections}")
	m.data.SetDataType(pdata.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pdata.MetricAggregationTemporalityCumulative)
}

type MetricMetadataElasticsearchNodeClusterConnections struct{}

func (m MetricMetadataElasticsearchNodeClusterConnections) GetName() string {
	return "elasticsearch.node.cluster.connections"
}

func (m MetricMetadataElasticsearchNodeClusterConnections) GetDescription() string {
	return "The number of open tcp connections for internal cluster communication."
}

func (m MetricMetadataElasticsearchNodeClusterConnections) GetUnit() string {
	return "{connections}"
}

func (m MetricMetadataElasticsearchNodeClusterConnections) GetValueType() string {
	return "int64"
}

func (m MetricMetadataElasticsearchNodeClusterConnections) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Sum: &Sum{
			Aggregation: pdata.MetricAggregationTemporalityCumulative,
			Monotonic:   false,
			ValueType:   "Int",
		},
	}
}

func (m *metricElasticsearchNodeClusterConnections) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeClusterConnections) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeClusterConnections) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeClusterConnections(settings MetricSettings) metricElasticsearchNodeClusterConnections {
	m := metricElasticsearchNodeClusterConnections{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeClusterIo struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.cluster.io metric with initial data.
func (m *metricElasticsearchNodeClusterIo) init() {
	m.data.SetName("elasticsearch.node.cluster.io")
	m.data.SetDescription("The number of bytes sent and received on the network for internal cluster communication.")
	m.data.SetUnit("By")
	m.data.SetDataType(pdata.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pdata.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

type MetricMetadataElasticsearchNodeClusterIo struct{}

func (m MetricMetadataElasticsearchNodeClusterIo) GetName() string {
	return "elasticsearch.node.cluster.io"
}

func (m MetricMetadataElasticsearchNodeClusterIo) GetDescription() string {
	return "The number of bytes sent and received on the network for internal cluster communication."
}

func (m MetricMetadataElasticsearchNodeClusterIo) GetUnit() string {
	return "By"
}

func (m MetricMetadataElasticsearchNodeClusterIo) GetValueType() string {
	return "int64"
}

func (m MetricMetadataElasticsearchNodeClusterIo) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Sum: &Sum{
			Aggregation: pdata.MetricAggregationTemporalityCumulative,
			Monotonic:   true,
			ValueType:   "Int",
		},
	}
}

func (m *metricElasticsearchNodeClusterIo) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64, directionAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Direction, pdata.NewValueString(directionAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeClusterIo) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeClusterIo) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeClusterIo(settings MetricSettings) metricElasticsearchNodeClusterIo {
	m := metricElasticsearchNodeClusterIo{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeDocuments struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.documents metric with initial data.
func (m *metricElasticsearchNodeDocuments) init() {
	m.data.SetName("elasticsearch.node.documents")
	m.data.SetDescription("The number of documents on the node.")
	m.data.SetUnit("{documents}")
	m.data.SetDataType(pdata.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pdata.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

type MetricMetadataElasticsearchNodeDocuments struct{}

func (m MetricMetadataElasticsearchNodeDocuments) GetName() string {
	return "elasticsearch.node.documents"
}

func (m MetricMetadataElasticsearchNodeDocuments) GetDescription() string {
	return "The number of documents on the node."
}

func (m MetricMetadataElasticsearchNodeDocuments) GetUnit() string {
	return "{documents}"
}

func (m MetricMetadataElasticsearchNodeDocuments) GetValueType() string {
	return "int64"
}

func (m MetricMetadataElasticsearchNodeDocuments) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Sum: &Sum{
			Aggregation: pdata.MetricAggregationTemporalityCumulative,
			Monotonic:   false,
			ValueType:   "Int",
		},
	}
}

func (m *metricElasticsearchNodeDocuments) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64, documentStateAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.DocumentState, pdata.NewValueString(documentStateAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeDocuments) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeDocuments) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeDocuments(settings MetricSettings) metricElasticsearchNodeDocuments {
	m := metricElasticsearchNodeDocuments{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeFsDiskAvailable struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.fs.disk.available metric with initial data.
func (m *metricElasticsearchNodeFsDiskAvailable) init() {
	m.data.SetName("elasticsearch.node.fs.disk.available")
	m.data.SetDescription("The amount of disk space available across all file stores for this node.")
	m.data.SetUnit("By")
	m.data.SetDataType(pdata.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pdata.MetricAggregationTemporalityCumulative)
}

type MetricMetadataElasticsearchNodeFsDiskAvailable struct{}

func (m MetricMetadataElasticsearchNodeFsDiskAvailable) GetName() string {
	return "elasticsearch.node.fs.disk.available"
}

func (m MetricMetadataElasticsearchNodeFsDiskAvailable) GetDescription() string {
	return "The amount of disk space available across all file stores for this node."
}

func (m MetricMetadataElasticsearchNodeFsDiskAvailable) GetUnit() string {
	return "By"
}

func (m MetricMetadataElasticsearchNodeFsDiskAvailable) GetValueType() string {
	return "int64"
}

func (m MetricMetadataElasticsearchNodeFsDiskAvailable) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Sum: &Sum{
			Aggregation: pdata.MetricAggregationTemporalityCumulative,
			Monotonic:   false,
			ValueType:   "Int",
		},
	}
}

func (m *metricElasticsearchNodeFsDiskAvailable) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeFsDiskAvailable) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeFsDiskAvailable) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeFsDiskAvailable(settings MetricSettings) metricElasticsearchNodeFsDiskAvailable {
	m := metricElasticsearchNodeFsDiskAvailable{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeHTTPConnections struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.http.connections metric with initial data.
func (m *metricElasticsearchNodeHTTPConnections) init() {
	m.data.SetName("elasticsearch.node.http.connections")
	m.data.SetDescription("The number of HTTP connections to the node.")
	m.data.SetUnit("{connections}")
	m.data.SetDataType(pdata.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pdata.MetricAggregationTemporalityCumulative)
}

type MetricMetadataElasticsearchNodeHTTPConnections struct{}

func (m MetricMetadataElasticsearchNodeHTTPConnections) GetName() string {
	return "elasticsearch.node.http.connections"
}

func (m MetricMetadataElasticsearchNodeHTTPConnections) GetDescription() string {
	return "The number of HTTP connections to the node."
}

func (m MetricMetadataElasticsearchNodeHTTPConnections) GetUnit() string {
	return "{connections}"
}

func (m MetricMetadataElasticsearchNodeHTTPConnections) GetValueType() string {
	return "int64"
}

func (m MetricMetadataElasticsearchNodeHTTPConnections) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Sum: &Sum{
			Aggregation: pdata.MetricAggregationTemporalityCumulative,
			Monotonic:   false,
			ValueType:   "Int",
		},
	}
}

func (m *metricElasticsearchNodeHTTPConnections) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeHTTPConnections) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeHTTPConnections) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeHTTPConnections(settings MetricSettings) metricElasticsearchNodeHTTPConnections {
	m := metricElasticsearchNodeHTTPConnections{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeOpenFiles struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.open_files metric with initial data.
func (m *metricElasticsearchNodeOpenFiles) init() {
	m.data.SetName("elasticsearch.node.open_files")
	m.data.SetDescription("The number of open file descriptors held by the node.")
	m.data.SetUnit("{files}")
	m.data.SetDataType(pdata.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pdata.MetricAggregationTemporalityCumulative)
}

type MetricMetadataElasticsearchNodeOpenFiles struct{}

func (m MetricMetadataElasticsearchNodeOpenFiles) GetName() string {
	return "elasticsearch.node.open_files"
}

func (m MetricMetadataElasticsearchNodeOpenFiles) GetDescription() string {
	return "The number of open file descriptors held by the node."
}

func (m MetricMetadataElasticsearchNodeOpenFiles) GetUnit() string {
	return "{files}"
}

func (m MetricMetadataElasticsearchNodeOpenFiles) GetValueType() string {
	return "int64"
}

func (m MetricMetadataElasticsearchNodeOpenFiles) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Sum: &Sum{
			Aggregation: pdata.MetricAggregationTemporalityCumulative,
			Monotonic:   false,
			ValueType:   "Int",
		},
	}
}

func (m *metricElasticsearchNodeOpenFiles) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeOpenFiles) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeOpenFiles) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeOpenFiles(settings MetricSettings) metricElasticsearchNodeOpenFiles {
	m := metricElasticsearchNodeOpenFiles{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeOperationsCompleted struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.operations.completed metric with initial data.
func (m *metricElasticsearchNodeOperationsCompleted) init() {
	m.data.SetName("elasticsearch.node.operations.completed")
	m.data.SetDescription("The number of operations completed.")
	m.data.SetUnit("{operations}")
	m.data.SetDataType(pdata.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pdata.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

type MetricMetadataElasticsearchNodeOperationsCompleted struct{}

func (m MetricMetadataElasticsearchNodeOperationsCompleted) GetName() string {
	return "elasticsearch.node.operations.completed"
}

func (m MetricMetadataElasticsearchNodeOperationsCompleted) GetDescription() string {
	return "The number of operations completed."
}

func (m MetricMetadataElasticsearchNodeOperationsCompleted) GetUnit() string {
	return "{operations}"
}

func (m MetricMetadataElasticsearchNodeOperationsCompleted) GetValueType() string {
	return "int64"
}

func (m MetricMetadataElasticsearchNodeOperationsCompleted) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Sum: &Sum{
			Aggregation: pdata.MetricAggregationTemporalityCumulative,
			Monotonic:   true,
			ValueType:   "Int",
		},
	}
}

func (m *metricElasticsearchNodeOperationsCompleted) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64, operationAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Operation, pdata.NewValueString(operationAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeOperationsCompleted) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeOperationsCompleted) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeOperationsCompleted(settings MetricSettings) metricElasticsearchNodeOperationsCompleted {
	m := metricElasticsearchNodeOperationsCompleted{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeOperationsTime struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.operations.time metric with initial data.
func (m *metricElasticsearchNodeOperationsTime) init() {
	m.data.SetName("elasticsearch.node.operations.time")
	m.data.SetDescription("Time spent on operations.")
	m.data.SetUnit("ms")
	m.data.SetDataType(pdata.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pdata.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

type MetricMetadataElasticsearchNodeOperationsTime struct{}

func (m MetricMetadataElasticsearchNodeOperationsTime) GetName() string {
	return "elasticsearch.node.operations.time"
}

func (m MetricMetadataElasticsearchNodeOperationsTime) GetDescription() string {
	return "Time spent on operations."
}

func (m MetricMetadataElasticsearchNodeOperationsTime) GetUnit() string {
	return "ms"
}

func (m MetricMetadataElasticsearchNodeOperationsTime) GetValueType() string {
	return "int64"
}

func (m MetricMetadataElasticsearchNodeOperationsTime) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Sum: &Sum{
			Aggregation: pdata.MetricAggregationTemporalityCumulative,
			Monotonic:   true,
			ValueType:   "Int",
		},
	}
}

func (m *metricElasticsearchNodeOperationsTime) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64, operationAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Operation, pdata.NewValueString(operationAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeOperationsTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeOperationsTime) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeOperationsTime(settings MetricSettings) metricElasticsearchNodeOperationsTime {
	m := metricElasticsearchNodeOperationsTime{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeShardsSize struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.shards.size metric with initial data.
func (m *metricElasticsearchNodeShardsSize) init() {
	m.data.SetName("elasticsearch.node.shards.size")
	m.data.SetDescription("The size of the shards assigned to this node.")
	m.data.SetUnit("By")
	m.data.SetDataType(pdata.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pdata.MetricAggregationTemporalityCumulative)
}

type MetricMetadataElasticsearchNodeShardsSize struct{}

func (m MetricMetadataElasticsearchNodeShardsSize) GetName() string {
	return "elasticsearch.node.shards.size"
}

func (m MetricMetadataElasticsearchNodeShardsSize) GetDescription() string {
	return "The size of the shards assigned to this node."
}

func (m MetricMetadataElasticsearchNodeShardsSize) GetUnit() string {
	return "By"
}

func (m MetricMetadataElasticsearchNodeShardsSize) GetValueType() string {
	return "int64"
}

func (m MetricMetadataElasticsearchNodeShardsSize) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Sum: &Sum{
			Aggregation: pdata.MetricAggregationTemporalityCumulative,
			Monotonic:   false,
			ValueType:   "Int",
		},
	}
}

func (m *metricElasticsearchNodeShardsSize) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeShardsSize) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeShardsSize) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeShardsSize(settings MetricSettings) metricElasticsearchNodeShardsSize {
	m := metricElasticsearchNodeShardsSize{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeThreadPoolTasksFinished struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.thread_pool.tasks.finished metric with initial data.
func (m *metricElasticsearchNodeThreadPoolTasksFinished) init() {
	m.data.SetName("elasticsearch.node.thread_pool.tasks.finished")
	m.data.SetDescription("The number of tasks finished by the thread pool.")
	m.data.SetUnit("{tasks}")
	m.data.SetDataType(pdata.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pdata.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

type MetricMetadataElasticsearchNodeThreadPoolTasksFinished struct{}

func (m MetricMetadataElasticsearchNodeThreadPoolTasksFinished) GetName() string {
	return "elasticsearch.node.thread_pool.tasks.finished"
}

func (m MetricMetadataElasticsearchNodeThreadPoolTasksFinished) GetDescription() string {
	return "The number of tasks finished by the thread pool."
}

func (m MetricMetadataElasticsearchNodeThreadPoolTasksFinished) GetUnit() string {
	return "{tasks}"
}

func (m MetricMetadataElasticsearchNodeThreadPoolTasksFinished) GetValueType() string {
	return "int64"
}

func (m MetricMetadataElasticsearchNodeThreadPoolTasksFinished) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Sum: &Sum{
			Aggregation: pdata.MetricAggregationTemporalityCumulative,
			Monotonic:   true,
			ValueType:   "Int",
		},
	}
}

func (m *metricElasticsearchNodeThreadPoolTasksFinished) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64, threadPoolNameAttributeValue string, taskStateAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.ThreadPoolName, pdata.NewValueString(threadPoolNameAttributeValue))
	dp.Attributes().Insert(A.TaskState, pdata.NewValueString(taskStateAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeThreadPoolTasksFinished) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeThreadPoolTasksFinished) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeThreadPoolTasksFinished(settings MetricSettings) metricElasticsearchNodeThreadPoolTasksFinished {
	m := metricElasticsearchNodeThreadPoolTasksFinished{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeThreadPoolTasksQueued struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.thread_pool.tasks.queued metric with initial data.
func (m *metricElasticsearchNodeThreadPoolTasksQueued) init() {
	m.data.SetName("elasticsearch.node.thread_pool.tasks.queued")
	m.data.SetDescription("The number of queued tasks in the thread pool.")
	m.data.SetUnit("{tasks}")
	m.data.SetDataType(pdata.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pdata.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

type MetricMetadataElasticsearchNodeThreadPoolTasksQueued struct{}

func (m MetricMetadataElasticsearchNodeThreadPoolTasksQueued) GetName() string {
	return "elasticsearch.node.thread_pool.tasks.queued"
}

func (m MetricMetadataElasticsearchNodeThreadPoolTasksQueued) GetDescription() string {
	return "The number of queued tasks in the thread pool."
}

func (m MetricMetadataElasticsearchNodeThreadPoolTasksQueued) GetUnit() string {
	return "{tasks}"
}

func (m MetricMetadataElasticsearchNodeThreadPoolTasksQueued) GetValueType() string {
	return "int64"
}

func (m MetricMetadataElasticsearchNodeThreadPoolTasksQueued) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Sum: &Sum{
			Aggregation: pdata.MetricAggregationTemporalityCumulative,
			Monotonic:   false,
			ValueType:   "Int",
		},
	}
}

func (m *metricElasticsearchNodeThreadPoolTasksQueued) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64, threadPoolNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.ThreadPoolName, pdata.NewValueString(threadPoolNameAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeThreadPoolTasksQueued) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeThreadPoolTasksQueued) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeThreadPoolTasksQueued(settings MetricSettings) metricElasticsearchNodeThreadPoolTasksQueued {
	m := metricElasticsearchNodeThreadPoolTasksQueued{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricElasticsearchNodeThreadPoolThreads struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills elasticsearch.node.thread_pool.threads metric with initial data.
func (m *metricElasticsearchNodeThreadPoolThreads) init() {
	m.data.SetName("elasticsearch.node.thread_pool.threads")
	m.data.SetDescription("The number of threads in the thread pool.")
	m.data.SetUnit("{threads}")
	m.data.SetDataType(pdata.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pdata.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

type MetricMetadataElasticsearchNodeThreadPoolThreads struct{}

func (m MetricMetadataElasticsearchNodeThreadPoolThreads) GetName() string {
	return "elasticsearch.node.thread_pool.threads"
}

func (m MetricMetadataElasticsearchNodeThreadPoolThreads) GetDescription() string {
	return "The number of threads in the thread pool."
}

func (m MetricMetadataElasticsearchNodeThreadPoolThreads) GetUnit() string {
	return "{threads}"
}

func (m MetricMetadataElasticsearchNodeThreadPoolThreads) GetValueType() string {
	return "int64"
}

func (m MetricMetadataElasticsearchNodeThreadPoolThreads) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Sum: &Sum{
			Aggregation: pdata.MetricAggregationTemporalityCumulative,
			Monotonic:   false,
			ValueType:   "Int",
		},
	}
}

func (m *metricElasticsearchNodeThreadPoolThreads) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64, threadPoolNameAttributeValue string, threadStateAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.ThreadPoolName, pdata.NewValueString(threadPoolNameAttributeValue))
	dp.Attributes().Insert(A.ThreadState, pdata.NewValueString(threadStateAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricElasticsearchNodeThreadPoolThreads) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricElasticsearchNodeThreadPoolThreads) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricElasticsearchNodeThreadPoolThreads(settings MetricSettings) metricElasticsearchNodeThreadPoolThreads {
	m := metricElasticsearchNodeThreadPoolThreads{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricJvmClassesLoaded struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills jvm.classes.loaded metric with initial data.
func (m *metricJvmClassesLoaded) init() {
	m.data.SetName("jvm.classes.loaded")
	m.data.SetDescription("The number of loaded classes")
	m.data.SetUnit("1")
	m.data.SetDataType(pdata.MetricDataTypeGauge)
}

type MetricMetadataJvmClassesLoaded struct{}

func (m MetricMetadataJvmClassesLoaded) GetName() string {
	return "jvm.classes.loaded"
}

func (m MetricMetadataJvmClassesLoaded) GetDescription() string {
	return "The number of loaded classes"
}

func (m MetricMetadataJvmClassesLoaded) GetUnit() string {
	return "1"
}

func (m MetricMetadataJvmClassesLoaded) GetValueType() string {
	return "int64"
}

func (m MetricMetadataJvmClassesLoaded) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Gauge: &Gauge{
			ValueType: "Int",
		},
	}
}

func (m *metricJvmClassesLoaded) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricJvmClassesLoaded) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricJvmClassesLoaded) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricJvmClassesLoaded(settings MetricSettings) metricJvmClassesLoaded {
	m := metricJvmClassesLoaded{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricJvmGcCollectionsCount struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills jvm.gc.collections.count metric with initial data.
func (m *metricJvmGcCollectionsCount) init() {
	m.data.SetName("jvm.gc.collections.count")
	m.data.SetDescription("The total number of garbage collections that have occurred")
	m.data.SetUnit("1")
	m.data.SetDataType(pdata.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pdata.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

type MetricMetadataJvmGcCollectionsCount struct{}

func (m MetricMetadataJvmGcCollectionsCount) GetName() string {
	return "jvm.gc.collections.count"
}

func (m MetricMetadataJvmGcCollectionsCount) GetDescription() string {
	return "The total number of garbage collections that have occurred"
}

func (m MetricMetadataJvmGcCollectionsCount) GetUnit() string {
	return "1"
}

func (m MetricMetadataJvmGcCollectionsCount) GetValueType() string {
	return "int64"
}

func (m MetricMetadataJvmGcCollectionsCount) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Sum: &Sum{
			Aggregation: pdata.MetricAggregationTemporalityCumulative,
			Monotonic:   true,
			ValueType:   "Int",
		},
	}
}

func (m *metricJvmGcCollectionsCount) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64, collectorNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.CollectorName, pdata.NewValueString(collectorNameAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricJvmGcCollectionsCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricJvmGcCollectionsCount) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricJvmGcCollectionsCount(settings MetricSettings) metricJvmGcCollectionsCount {
	m := metricJvmGcCollectionsCount{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricJvmGcCollectionsElapsed struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills jvm.gc.collections.elapsed metric with initial data.
func (m *metricJvmGcCollectionsElapsed) init() {
	m.data.SetName("jvm.gc.collections.elapsed")
	m.data.SetDescription("The approximate accumulated collection elapsed time")
	m.data.SetUnit("ms")
	m.data.SetDataType(pdata.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pdata.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

type MetricMetadataJvmGcCollectionsElapsed struct{}

func (m MetricMetadataJvmGcCollectionsElapsed) GetName() string {
	return "jvm.gc.collections.elapsed"
}

func (m MetricMetadataJvmGcCollectionsElapsed) GetDescription() string {
	return "The approximate accumulated collection elapsed time"
}

func (m MetricMetadataJvmGcCollectionsElapsed) GetUnit() string {
	return "ms"
}

func (m MetricMetadataJvmGcCollectionsElapsed) GetValueType() string {
	return "int64"
}

func (m MetricMetadataJvmGcCollectionsElapsed) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Sum: &Sum{
			Aggregation: pdata.MetricAggregationTemporalityCumulative,
			Monotonic:   true,
			ValueType:   "Int",
		},
	}
}

func (m *metricJvmGcCollectionsElapsed) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64, collectorNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.CollectorName, pdata.NewValueString(collectorNameAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricJvmGcCollectionsElapsed) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricJvmGcCollectionsElapsed) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricJvmGcCollectionsElapsed(settings MetricSettings) metricJvmGcCollectionsElapsed {
	m := metricJvmGcCollectionsElapsed{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricJvmMemoryHeapCommitted struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills jvm.memory.heap.committed metric with initial data.
func (m *metricJvmMemoryHeapCommitted) init() {
	m.data.SetName("jvm.memory.heap.committed")
	m.data.SetDescription("The amount of memory that is guaranteed to be available for the heap")
	m.data.SetUnit("By")
	m.data.SetDataType(pdata.MetricDataTypeGauge)
}

type MetricMetadataJvmMemoryHeapCommitted struct{}

func (m MetricMetadataJvmMemoryHeapCommitted) GetName() string {
	return "jvm.memory.heap.committed"
}

func (m MetricMetadataJvmMemoryHeapCommitted) GetDescription() string {
	return "The amount of memory that is guaranteed to be available for the heap"
}

func (m MetricMetadataJvmMemoryHeapCommitted) GetUnit() string {
	return "By"
}

func (m MetricMetadataJvmMemoryHeapCommitted) GetValueType() string {
	return "int64"
}

func (m MetricMetadataJvmMemoryHeapCommitted) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Gauge: &Gauge{
			ValueType: "Int",
		},
	}
}

func (m *metricJvmMemoryHeapCommitted) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricJvmMemoryHeapCommitted) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricJvmMemoryHeapCommitted) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricJvmMemoryHeapCommitted(settings MetricSettings) metricJvmMemoryHeapCommitted {
	m := metricJvmMemoryHeapCommitted{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricJvmMemoryHeapMax struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills jvm.memory.heap.max metric with initial data.
func (m *metricJvmMemoryHeapMax) init() {
	m.data.SetName("jvm.memory.heap.max")
	m.data.SetDescription("The maximum amount of memory can be used for the heap")
	m.data.SetUnit("By")
	m.data.SetDataType(pdata.MetricDataTypeGauge)
}

type MetricMetadataJvmMemoryHeapMax struct{}

func (m MetricMetadataJvmMemoryHeapMax) GetName() string {
	return "jvm.memory.heap.max"
}

func (m MetricMetadataJvmMemoryHeapMax) GetDescription() string {
	return "The maximum amount of memory can be used for the heap"
}

func (m MetricMetadataJvmMemoryHeapMax) GetUnit() string {
	return "By"
}

func (m MetricMetadataJvmMemoryHeapMax) GetValueType() string {
	return "int64"
}

func (m MetricMetadataJvmMemoryHeapMax) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Gauge: &Gauge{
			ValueType: "Int",
		},
	}
}

func (m *metricJvmMemoryHeapMax) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricJvmMemoryHeapMax) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricJvmMemoryHeapMax) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricJvmMemoryHeapMax(settings MetricSettings) metricJvmMemoryHeapMax {
	m := metricJvmMemoryHeapMax{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricJvmMemoryHeapUsed struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills jvm.memory.heap.used metric with initial data.
func (m *metricJvmMemoryHeapUsed) init() {
	m.data.SetName("jvm.memory.heap.used")
	m.data.SetDescription("The current heap memory usage")
	m.data.SetUnit("By")
	m.data.SetDataType(pdata.MetricDataTypeGauge)
}

type MetricMetadataJvmMemoryHeapUsed struct{}

func (m MetricMetadataJvmMemoryHeapUsed) GetName() string {
	return "jvm.memory.heap.used"
}

func (m MetricMetadataJvmMemoryHeapUsed) GetDescription() string {
	return "The current heap memory usage"
}

func (m MetricMetadataJvmMemoryHeapUsed) GetUnit() string {
	return "By"
}

func (m MetricMetadataJvmMemoryHeapUsed) GetValueType() string {
	return "int64"
}

func (m MetricMetadataJvmMemoryHeapUsed) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Gauge: &Gauge{
			ValueType: "Int",
		},
	}
}

func (m *metricJvmMemoryHeapUsed) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricJvmMemoryHeapUsed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricJvmMemoryHeapUsed) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricJvmMemoryHeapUsed(settings MetricSettings) metricJvmMemoryHeapUsed {
	m := metricJvmMemoryHeapUsed{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricJvmMemoryNonheapCommitted struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills jvm.memory.nonheap.committed metric with initial data.
func (m *metricJvmMemoryNonheapCommitted) init() {
	m.data.SetName("jvm.memory.nonheap.committed")
	m.data.SetDescription("The amount of memory that is guaranteed to be available for non-heap purposes")
	m.data.SetUnit("By")
	m.data.SetDataType(pdata.MetricDataTypeGauge)
}

type MetricMetadataJvmMemoryNonheapCommitted struct{}

func (m MetricMetadataJvmMemoryNonheapCommitted) GetName() string {
	return "jvm.memory.nonheap.committed"
}

func (m MetricMetadataJvmMemoryNonheapCommitted) GetDescription() string {
	return "The amount of memory that is guaranteed to be available for non-heap purposes"
}

func (m MetricMetadataJvmMemoryNonheapCommitted) GetUnit() string {
	return "By"
}

func (m MetricMetadataJvmMemoryNonheapCommitted) GetValueType() string {
	return "int64"
}

func (m MetricMetadataJvmMemoryNonheapCommitted) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Gauge: &Gauge{
			ValueType: "Int",
		},
	}
}

func (m *metricJvmMemoryNonheapCommitted) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricJvmMemoryNonheapCommitted) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricJvmMemoryNonheapCommitted) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricJvmMemoryNonheapCommitted(settings MetricSettings) metricJvmMemoryNonheapCommitted {
	m := metricJvmMemoryNonheapCommitted{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricJvmMemoryNonheapUsed struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills jvm.memory.nonheap.used metric with initial data.
func (m *metricJvmMemoryNonheapUsed) init() {
	m.data.SetName("jvm.memory.nonheap.used")
	m.data.SetDescription("The current non-heap memory usage")
	m.data.SetUnit("By")
	m.data.SetDataType(pdata.MetricDataTypeGauge)
}

type MetricMetadataJvmMemoryNonheapUsed struct{}

func (m MetricMetadataJvmMemoryNonheapUsed) GetName() string {
	return "jvm.memory.nonheap.used"
}

func (m MetricMetadataJvmMemoryNonheapUsed) GetDescription() string {
	return "The current non-heap memory usage"
}

func (m MetricMetadataJvmMemoryNonheapUsed) GetUnit() string {
	return "By"
}

func (m MetricMetadataJvmMemoryNonheapUsed) GetValueType() string {
	return "int64"
}

func (m MetricMetadataJvmMemoryNonheapUsed) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Gauge: &Gauge{
			ValueType: "Int",
		},
	}
}

func (m *metricJvmMemoryNonheapUsed) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricJvmMemoryNonheapUsed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricJvmMemoryNonheapUsed) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricJvmMemoryNonheapUsed(settings MetricSettings) metricJvmMemoryNonheapUsed {
	m := metricJvmMemoryNonheapUsed{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricJvmMemoryPoolMax struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills jvm.memory.pool.max metric with initial data.
func (m *metricJvmMemoryPoolMax) init() {
	m.data.SetName("jvm.memory.pool.max")
	m.data.SetDescription("The maximum amount of memory can be used for the memory pool")
	m.data.SetUnit("By")
	m.data.SetDataType(pdata.MetricDataTypeGauge)
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

type MetricMetadataJvmMemoryPoolMax struct{}

func (m MetricMetadataJvmMemoryPoolMax) GetName() string {
	return "jvm.memory.pool.max"
}

func (m MetricMetadataJvmMemoryPoolMax) GetDescription() string {
	return "The maximum amount of memory can be used for the memory pool"
}

func (m MetricMetadataJvmMemoryPoolMax) GetUnit() string {
	return "By"
}

func (m MetricMetadataJvmMemoryPoolMax) GetValueType() string {
	return "int64"
}

func (m MetricMetadataJvmMemoryPoolMax) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Gauge: &Gauge{
			ValueType: "Int",
		},
	}
}

func (m *metricJvmMemoryPoolMax) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64, memoryPoolNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.MemoryPoolName, pdata.NewValueString(memoryPoolNameAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricJvmMemoryPoolMax) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricJvmMemoryPoolMax) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricJvmMemoryPoolMax(settings MetricSettings) metricJvmMemoryPoolMax {
	m := metricJvmMemoryPoolMax{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricJvmMemoryPoolUsed struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills jvm.memory.pool.used metric with initial data.
func (m *metricJvmMemoryPoolUsed) init() {
	m.data.SetName("jvm.memory.pool.used")
	m.data.SetDescription("The current memory pool memory usage")
	m.data.SetUnit("By")
	m.data.SetDataType(pdata.MetricDataTypeGauge)
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

type MetricMetadataJvmMemoryPoolUsed struct{}

func (m MetricMetadataJvmMemoryPoolUsed) GetName() string {
	return "jvm.memory.pool.used"
}

func (m MetricMetadataJvmMemoryPoolUsed) GetDescription() string {
	return "The current memory pool memory usage"
}

func (m MetricMetadataJvmMemoryPoolUsed) GetUnit() string {
	return "By"
}

func (m MetricMetadataJvmMemoryPoolUsed) GetValueType() string {
	return "int64"
}

func (m MetricMetadataJvmMemoryPoolUsed) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Gauge: &Gauge{
			ValueType: "Int",
		},
	}
}

func (m *metricJvmMemoryPoolUsed) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64, memoryPoolNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.MemoryPoolName, pdata.NewValueString(memoryPoolNameAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricJvmMemoryPoolUsed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricJvmMemoryPoolUsed) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricJvmMemoryPoolUsed(settings MetricSettings) metricJvmMemoryPoolUsed {
	m := metricJvmMemoryPoolUsed{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricJvmThreadsCount struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills jvm.threads.count metric with initial data.
func (m *metricJvmThreadsCount) init() {
	m.data.SetName("jvm.threads.count")
	m.data.SetDescription("The current number of threads")
	m.data.SetUnit("1")
	m.data.SetDataType(pdata.MetricDataTypeGauge)
}

type MetricMetadataJvmThreadsCount struct{}

func (m MetricMetadataJvmThreadsCount) GetName() string {
	return "jvm.threads.count"
}

func (m MetricMetadataJvmThreadsCount) GetDescription() string {
	return "The current number of threads"
}

func (m MetricMetadataJvmThreadsCount) GetUnit() string {
	return "1"
}

func (m MetricMetadataJvmThreadsCount) GetValueType() string {
	return "int64"
}

func (m MetricMetadataJvmThreadsCount) GetMetricType() MetricDataTypeMetadata {
	return MetricDataTypeMetadata{
		Gauge: &Gauge{
			ValueType: "Int",
		},
	}
}

func (m *metricJvmThreadsCount) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricJvmThreadsCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricJvmThreadsCount) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricJvmThreadsCount(settings MetricSettings) metricJvmThreadsCount {
	m := metricJvmThreadsCount{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

// MetricsBuilder provides an interface for scrapers to report metrics while taking care of all the transformations
// required to produce metric representation defined in metadata and user settings.
type MetricsBuilder struct {
	startTime                                      pdata.Timestamp // start time that will be applied to all recorded data points.
	metricsCapacity                                int             // maximum observed number of metrics per resource.
	resourceCapacity                               int             // maximum observed number of resource attributes.
	metricsBuffer                                  pdata.Metrics   // accumulates metrics data before emitting.
	metricElasticsearchClusterDataNodes            metricElasticsearchClusterDataNodes
	metricElasticsearchClusterHealth               metricElasticsearchClusterHealth
	metricElasticsearchClusterNodes                metricElasticsearchClusterNodes
	metricElasticsearchClusterShards               metricElasticsearchClusterShards
	metricElasticsearchNodeCacheEvictions          metricElasticsearchNodeCacheEvictions
	metricElasticsearchNodeCacheMemoryUsage        metricElasticsearchNodeCacheMemoryUsage
	metricElasticsearchNodeClusterConnections      metricElasticsearchNodeClusterConnections
	metricElasticsearchNodeClusterIo               metricElasticsearchNodeClusterIo
	metricElasticsearchNodeDocuments               metricElasticsearchNodeDocuments
	metricElasticsearchNodeFsDiskAvailable         metricElasticsearchNodeFsDiskAvailable
	metricElasticsearchNodeHTTPConnections         metricElasticsearchNodeHTTPConnections
	metricElasticsearchNodeOpenFiles               metricElasticsearchNodeOpenFiles
	metricElasticsearchNodeOperationsCompleted     metricElasticsearchNodeOperationsCompleted
	metricElasticsearchNodeOperationsTime          metricElasticsearchNodeOperationsTime
	metricElasticsearchNodeShardsSize              metricElasticsearchNodeShardsSize
	metricElasticsearchNodeThreadPoolTasksFinished metricElasticsearchNodeThreadPoolTasksFinished
	metricElasticsearchNodeThreadPoolTasksQueued   metricElasticsearchNodeThreadPoolTasksQueued
	metricElasticsearchNodeThreadPoolThreads       metricElasticsearchNodeThreadPoolThreads
	metricJvmClassesLoaded                         metricJvmClassesLoaded
	metricJvmGcCollectionsCount                    metricJvmGcCollectionsCount
	metricJvmGcCollectionsElapsed                  metricJvmGcCollectionsElapsed
	metricJvmMemoryHeapCommitted                   metricJvmMemoryHeapCommitted
	metricJvmMemoryHeapMax                         metricJvmMemoryHeapMax
	metricJvmMemoryHeapUsed                        metricJvmMemoryHeapUsed
	metricJvmMemoryNonheapCommitted                metricJvmMemoryNonheapCommitted
	metricJvmMemoryNonheapUsed                     metricJvmMemoryNonheapUsed
	metricJvmMemoryPoolMax                         metricJvmMemoryPoolMax
	metricJvmMemoryPoolUsed                        metricJvmMemoryPoolUsed
	metricJvmThreadsCount                          metricJvmThreadsCount
}

// metricBuilderOption applies changes to default metrics builder.
type metricBuilderOption func(*MetricsBuilder)

// WithStartTime sets startTime on the metrics builder.
func WithStartTime(startTime pdata.Timestamp) metricBuilderOption {
	return func(mb *MetricsBuilder) {
		mb.startTime = startTime
	}
}

func NewMetricsBuilder(settings MetricsSettings, options ...metricBuilderOption) *MetricsBuilder {
	mb := &MetricsBuilder{
		startTime:                                      pdata.NewTimestampFromTime(time.Now()),
		metricsBuffer:                                  pdata.NewMetrics(),
		metricElasticsearchClusterDataNodes:            newMetricElasticsearchClusterDataNodes(settings.ElasticsearchClusterDataNodes),
		metricElasticsearchClusterHealth:               newMetricElasticsearchClusterHealth(settings.ElasticsearchClusterHealth),
		metricElasticsearchClusterNodes:                newMetricElasticsearchClusterNodes(settings.ElasticsearchClusterNodes),
		metricElasticsearchClusterShards:               newMetricElasticsearchClusterShards(settings.ElasticsearchClusterShards),
		metricElasticsearchNodeCacheEvictions:          newMetricElasticsearchNodeCacheEvictions(settings.ElasticsearchNodeCacheEvictions),
		metricElasticsearchNodeCacheMemoryUsage:        newMetricElasticsearchNodeCacheMemoryUsage(settings.ElasticsearchNodeCacheMemoryUsage),
		metricElasticsearchNodeClusterConnections:      newMetricElasticsearchNodeClusterConnections(settings.ElasticsearchNodeClusterConnections),
		metricElasticsearchNodeClusterIo:               newMetricElasticsearchNodeClusterIo(settings.ElasticsearchNodeClusterIo),
		metricElasticsearchNodeDocuments:               newMetricElasticsearchNodeDocuments(settings.ElasticsearchNodeDocuments),
		metricElasticsearchNodeFsDiskAvailable:         newMetricElasticsearchNodeFsDiskAvailable(settings.ElasticsearchNodeFsDiskAvailable),
		metricElasticsearchNodeHTTPConnections:         newMetricElasticsearchNodeHTTPConnections(settings.ElasticsearchNodeHTTPConnections),
		metricElasticsearchNodeOpenFiles:               newMetricElasticsearchNodeOpenFiles(settings.ElasticsearchNodeOpenFiles),
		metricElasticsearchNodeOperationsCompleted:     newMetricElasticsearchNodeOperationsCompleted(settings.ElasticsearchNodeOperationsCompleted),
		metricElasticsearchNodeOperationsTime:          newMetricElasticsearchNodeOperationsTime(settings.ElasticsearchNodeOperationsTime),
		metricElasticsearchNodeShardsSize:              newMetricElasticsearchNodeShardsSize(settings.ElasticsearchNodeShardsSize),
		metricElasticsearchNodeThreadPoolTasksFinished: newMetricElasticsearchNodeThreadPoolTasksFinished(settings.ElasticsearchNodeThreadPoolTasksFinished),
		metricElasticsearchNodeThreadPoolTasksQueued:   newMetricElasticsearchNodeThreadPoolTasksQueued(settings.ElasticsearchNodeThreadPoolTasksQueued),
		metricElasticsearchNodeThreadPoolThreads:       newMetricElasticsearchNodeThreadPoolThreads(settings.ElasticsearchNodeThreadPoolThreads),
		metricJvmClassesLoaded:                         newMetricJvmClassesLoaded(settings.JvmClassesLoaded),
		metricJvmGcCollectionsCount:                    newMetricJvmGcCollectionsCount(settings.JvmGcCollectionsCount),
		metricJvmGcCollectionsElapsed:                  newMetricJvmGcCollectionsElapsed(settings.JvmGcCollectionsElapsed),
		metricJvmMemoryHeapCommitted:                   newMetricJvmMemoryHeapCommitted(settings.JvmMemoryHeapCommitted),
		metricJvmMemoryHeapMax:                         newMetricJvmMemoryHeapMax(settings.JvmMemoryHeapMax),
		metricJvmMemoryHeapUsed:                        newMetricJvmMemoryHeapUsed(settings.JvmMemoryHeapUsed),
		metricJvmMemoryNonheapCommitted:                newMetricJvmMemoryNonheapCommitted(settings.JvmMemoryNonheapCommitted),
		metricJvmMemoryNonheapUsed:                     newMetricJvmMemoryNonheapUsed(settings.JvmMemoryNonheapUsed),
		metricJvmMemoryPoolMax:                         newMetricJvmMemoryPoolMax(settings.JvmMemoryPoolMax),
		metricJvmMemoryPoolUsed:                        newMetricJvmMemoryPoolUsed(settings.JvmMemoryPoolUsed),
		metricJvmThreadsCount:                          newMetricJvmThreadsCount(settings.JvmThreadsCount),
	}
	for _, op := range options {
		op(mb)
	}
	return mb
}

// updateCapacity updates max length of metrics and resource attributes that will be used for the slice capacity.
func (mb *MetricsBuilder) updateCapacity(rm pdata.ResourceMetrics) {
	if mb.metricsCapacity < rm.ScopeMetrics().At(0).Metrics().Len() {
		mb.metricsCapacity = rm.ScopeMetrics().At(0).Metrics().Len()
	}
	if mb.resourceCapacity < rm.Resource().Attributes().Len() {
		mb.resourceCapacity = rm.Resource().Attributes().Len()
	}
}

// ResourceOption applies changes to provided resource.
type ResourceOption func(pdata.Resource)

// EmitForResource saves all the generated metrics under a new resource and updates the internal state to be ready for
// recording another set of data points as part of another resource. This function can be helpful when one scraper
// needs to emit metrics from several resources. Otherwise calling this function is not required,
// just `Emit` function can be called instead. Resource attributes should be provided as ResourceOption arguments.
func (mb *MetricsBuilder) EmitForResource(ro ...ResourceOption) {
	rm := pdata.NewResourceMetrics()
	rm.Resource().Attributes().EnsureCapacity(mb.resourceCapacity)
	for _, op := range ro {
		op(rm.Resource())
	}
	ils := rm.ScopeMetrics().AppendEmpty()
	ils.Scope().SetName("otelcol/elasticsearchreceiver")
	ils.Metrics().EnsureCapacity(mb.metricsCapacity)
	mb.metricElasticsearchClusterDataNodes.emit(ils.Metrics())
	mb.metricElasticsearchClusterHealth.emit(ils.Metrics())
	mb.metricElasticsearchClusterNodes.emit(ils.Metrics())
	mb.metricElasticsearchClusterShards.emit(ils.Metrics())
	mb.metricElasticsearchNodeCacheEvictions.emit(ils.Metrics())
	mb.metricElasticsearchNodeCacheMemoryUsage.emit(ils.Metrics())
	mb.metricElasticsearchNodeClusterConnections.emit(ils.Metrics())
	mb.metricElasticsearchNodeClusterIo.emit(ils.Metrics())
	mb.metricElasticsearchNodeDocuments.emit(ils.Metrics())
	mb.metricElasticsearchNodeFsDiskAvailable.emit(ils.Metrics())
	mb.metricElasticsearchNodeHTTPConnections.emit(ils.Metrics())
	mb.metricElasticsearchNodeOpenFiles.emit(ils.Metrics())
	mb.metricElasticsearchNodeOperationsCompleted.emit(ils.Metrics())
	mb.metricElasticsearchNodeOperationsTime.emit(ils.Metrics())
	mb.metricElasticsearchNodeShardsSize.emit(ils.Metrics())
	mb.metricElasticsearchNodeThreadPoolTasksFinished.emit(ils.Metrics())
	mb.metricElasticsearchNodeThreadPoolTasksQueued.emit(ils.Metrics())
	mb.metricElasticsearchNodeThreadPoolThreads.emit(ils.Metrics())
	mb.metricJvmClassesLoaded.emit(ils.Metrics())
	mb.metricJvmGcCollectionsCount.emit(ils.Metrics())
	mb.metricJvmGcCollectionsElapsed.emit(ils.Metrics())
	mb.metricJvmMemoryHeapCommitted.emit(ils.Metrics())
	mb.metricJvmMemoryHeapMax.emit(ils.Metrics())
	mb.metricJvmMemoryHeapUsed.emit(ils.Metrics())
	mb.metricJvmMemoryNonheapCommitted.emit(ils.Metrics())
	mb.metricJvmMemoryNonheapUsed.emit(ils.Metrics())
	mb.metricJvmMemoryPoolMax.emit(ils.Metrics())
	mb.metricJvmMemoryPoolUsed.emit(ils.Metrics())
	mb.metricJvmThreadsCount.emit(ils.Metrics())
	if ils.Metrics().Len() > 0 {
		mb.updateCapacity(rm)
		rm.MoveTo(mb.metricsBuffer.ResourceMetrics().AppendEmpty())
	}
}

// Emit returns all the metrics accumulated by the metrics builder and updates the internal state to be ready for
// recording another set of metrics. This function will be responsible for applying all the transformations required to
// produce metric representation defined in metadata and user settings, e.g. delta or cumulative.
func (mb *MetricsBuilder) Emit(ro ...ResourceOption) pdata.Metrics {
	mb.EmitForResource(ro...)
	metrics := pdata.NewMetrics()
	mb.metricsBuffer.MoveTo(metrics)
	return metrics
}

// RecordElasticsearchClusterDataNodesDataPoint adds a data point to elasticsearch.cluster.data_nodes metric.
func (mb *MetricsBuilder) RecordElasticsearchClusterDataNodesDataPoint(ts pdata.Timestamp, val int64) {
	mb.metricElasticsearchClusterDataNodes.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchClusterHealthDataPoint adds a data point to elasticsearch.cluster.health metric.
func (mb *MetricsBuilder) RecordElasticsearchClusterHealthDataPoint(ts pdata.Timestamp, val int64, healthStatusAttributeValue string) {
	mb.metricElasticsearchClusterHealth.recordDataPoint(mb.startTime, ts, val, healthStatusAttributeValue)
}

// RecordElasticsearchClusterNodesDataPoint adds a data point to elasticsearch.cluster.nodes metric.
func (mb *MetricsBuilder) RecordElasticsearchClusterNodesDataPoint(ts pdata.Timestamp, val int64) {
	mb.metricElasticsearchClusterNodes.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchClusterShardsDataPoint adds a data point to elasticsearch.cluster.shards metric.
func (mb *MetricsBuilder) RecordElasticsearchClusterShardsDataPoint(ts pdata.Timestamp, val int64, shardStateAttributeValue string) {
	mb.metricElasticsearchClusterShards.recordDataPoint(mb.startTime, ts, val, shardStateAttributeValue)
}

// RecordElasticsearchNodeCacheEvictionsDataPoint adds a data point to elasticsearch.node.cache.evictions metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeCacheEvictionsDataPoint(ts pdata.Timestamp, val int64, cacheNameAttributeValue string) {
	mb.metricElasticsearchNodeCacheEvictions.recordDataPoint(mb.startTime, ts, val, cacheNameAttributeValue)
}

// RecordElasticsearchNodeCacheMemoryUsageDataPoint adds a data point to elasticsearch.node.cache.memory.usage metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeCacheMemoryUsageDataPoint(ts pdata.Timestamp, val int64, cacheNameAttributeValue string) {
	mb.metricElasticsearchNodeCacheMemoryUsage.recordDataPoint(mb.startTime, ts, val, cacheNameAttributeValue)
}

// RecordElasticsearchNodeClusterConnectionsDataPoint adds a data point to elasticsearch.node.cluster.connections metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeClusterConnectionsDataPoint(ts pdata.Timestamp, val int64) {
	mb.metricElasticsearchNodeClusterConnections.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeClusterIoDataPoint adds a data point to elasticsearch.node.cluster.io metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeClusterIoDataPoint(ts pdata.Timestamp, val int64, directionAttributeValue string) {
	mb.metricElasticsearchNodeClusterIo.recordDataPoint(mb.startTime, ts, val, directionAttributeValue)
}

// RecordElasticsearchNodeDocumentsDataPoint adds a data point to elasticsearch.node.documents metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeDocumentsDataPoint(ts pdata.Timestamp, val int64, documentStateAttributeValue string) {
	mb.metricElasticsearchNodeDocuments.recordDataPoint(mb.startTime, ts, val, documentStateAttributeValue)
}

// RecordElasticsearchNodeFsDiskAvailableDataPoint adds a data point to elasticsearch.node.fs.disk.available metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeFsDiskAvailableDataPoint(ts pdata.Timestamp, val int64) {
	mb.metricElasticsearchNodeFsDiskAvailable.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeHTTPConnectionsDataPoint adds a data point to elasticsearch.node.http.connections metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeHTTPConnectionsDataPoint(ts pdata.Timestamp, val int64) {
	mb.metricElasticsearchNodeHTTPConnections.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeOpenFilesDataPoint adds a data point to elasticsearch.node.open_files metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeOpenFilesDataPoint(ts pdata.Timestamp, val int64) {
	mb.metricElasticsearchNodeOpenFiles.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeOperationsCompletedDataPoint adds a data point to elasticsearch.node.operations.completed metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeOperationsCompletedDataPoint(ts pdata.Timestamp, val int64, operationAttributeValue string) {
	mb.metricElasticsearchNodeOperationsCompleted.recordDataPoint(mb.startTime, ts, val, operationAttributeValue)
}

// RecordElasticsearchNodeOperationsTimeDataPoint adds a data point to elasticsearch.node.operations.time metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeOperationsTimeDataPoint(ts pdata.Timestamp, val int64, operationAttributeValue string) {
	mb.metricElasticsearchNodeOperationsTime.recordDataPoint(mb.startTime, ts, val, operationAttributeValue)
}

// RecordElasticsearchNodeShardsSizeDataPoint adds a data point to elasticsearch.node.shards.size metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeShardsSizeDataPoint(ts pdata.Timestamp, val int64) {
	mb.metricElasticsearchNodeShardsSize.recordDataPoint(mb.startTime, ts, val)
}

// RecordElasticsearchNodeThreadPoolTasksFinishedDataPoint adds a data point to elasticsearch.node.thread_pool.tasks.finished metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeThreadPoolTasksFinishedDataPoint(ts pdata.Timestamp, val int64, threadPoolNameAttributeValue string, taskStateAttributeValue string) {
	mb.metricElasticsearchNodeThreadPoolTasksFinished.recordDataPoint(mb.startTime, ts, val, threadPoolNameAttributeValue, taskStateAttributeValue)
}

// RecordElasticsearchNodeThreadPoolTasksQueuedDataPoint adds a data point to elasticsearch.node.thread_pool.tasks.queued metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeThreadPoolTasksQueuedDataPoint(ts pdata.Timestamp, val int64, threadPoolNameAttributeValue string) {
	mb.metricElasticsearchNodeThreadPoolTasksQueued.recordDataPoint(mb.startTime, ts, val, threadPoolNameAttributeValue)
}

// RecordElasticsearchNodeThreadPoolThreadsDataPoint adds a data point to elasticsearch.node.thread_pool.threads metric.
func (mb *MetricsBuilder) RecordElasticsearchNodeThreadPoolThreadsDataPoint(ts pdata.Timestamp, val int64, threadPoolNameAttributeValue string, threadStateAttributeValue string) {
	mb.metricElasticsearchNodeThreadPoolThreads.recordDataPoint(mb.startTime, ts, val, threadPoolNameAttributeValue, threadStateAttributeValue)
}

// RecordJvmClassesLoadedDataPoint adds a data point to jvm.classes.loaded metric.
func (mb *MetricsBuilder) RecordJvmClassesLoadedDataPoint(ts pdata.Timestamp, val int64) {
	mb.metricJvmClassesLoaded.recordDataPoint(mb.startTime, ts, val)
}

// RecordJvmGcCollectionsCountDataPoint adds a data point to jvm.gc.collections.count metric.
func (mb *MetricsBuilder) RecordJvmGcCollectionsCountDataPoint(ts pdata.Timestamp, val int64, collectorNameAttributeValue string) {
	mb.metricJvmGcCollectionsCount.recordDataPoint(mb.startTime, ts, val, collectorNameAttributeValue)
}

// RecordJvmGcCollectionsElapsedDataPoint adds a data point to jvm.gc.collections.elapsed metric.
func (mb *MetricsBuilder) RecordJvmGcCollectionsElapsedDataPoint(ts pdata.Timestamp, val int64, collectorNameAttributeValue string) {
	mb.metricJvmGcCollectionsElapsed.recordDataPoint(mb.startTime, ts, val, collectorNameAttributeValue)
}

// RecordJvmMemoryHeapCommittedDataPoint adds a data point to jvm.memory.heap.committed metric.
func (mb *MetricsBuilder) RecordJvmMemoryHeapCommittedDataPoint(ts pdata.Timestamp, val int64) {
	mb.metricJvmMemoryHeapCommitted.recordDataPoint(mb.startTime, ts, val)
}

// RecordJvmMemoryHeapMaxDataPoint adds a data point to jvm.memory.heap.max metric.
func (mb *MetricsBuilder) RecordJvmMemoryHeapMaxDataPoint(ts pdata.Timestamp, val int64) {
	mb.metricJvmMemoryHeapMax.recordDataPoint(mb.startTime, ts, val)
}

// RecordJvmMemoryHeapUsedDataPoint adds a data point to jvm.memory.heap.used metric.
func (mb *MetricsBuilder) RecordJvmMemoryHeapUsedDataPoint(ts pdata.Timestamp, val int64) {
	mb.metricJvmMemoryHeapUsed.recordDataPoint(mb.startTime, ts, val)
}

// RecordJvmMemoryNonheapCommittedDataPoint adds a data point to jvm.memory.nonheap.committed metric.
func (mb *MetricsBuilder) RecordJvmMemoryNonheapCommittedDataPoint(ts pdata.Timestamp, val int64) {
	mb.metricJvmMemoryNonheapCommitted.recordDataPoint(mb.startTime, ts, val)
}

// RecordJvmMemoryNonheapUsedDataPoint adds a data point to jvm.memory.nonheap.used metric.
func (mb *MetricsBuilder) RecordJvmMemoryNonheapUsedDataPoint(ts pdata.Timestamp, val int64) {
	mb.metricJvmMemoryNonheapUsed.recordDataPoint(mb.startTime, ts, val)
}

// RecordJvmMemoryPoolMaxDataPoint adds a data point to jvm.memory.pool.max metric.
func (mb *MetricsBuilder) RecordJvmMemoryPoolMaxDataPoint(ts pdata.Timestamp, val int64, memoryPoolNameAttributeValue string) {
	mb.metricJvmMemoryPoolMax.recordDataPoint(mb.startTime, ts, val, memoryPoolNameAttributeValue)
}

// RecordJvmMemoryPoolUsedDataPoint adds a data point to jvm.memory.pool.used metric.
func (mb *MetricsBuilder) RecordJvmMemoryPoolUsedDataPoint(ts pdata.Timestamp, val int64, memoryPoolNameAttributeValue string) {
	mb.metricJvmMemoryPoolUsed.recordDataPoint(mb.startTime, ts, val, memoryPoolNameAttributeValue)
}

// RecordJvmThreadsCountDataPoint adds a data point to jvm.threads.count metric.
func (mb *MetricsBuilder) RecordJvmThreadsCountDataPoint(ts pdata.Timestamp, val int64) {
	mb.metricJvmThreadsCount.recordDataPoint(mb.startTime, ts, val)
}

// Reset resets metrics builder to its initial state. It should be used when external metrics source is restarted,
// and metrics builder should update its startTime and reset it's internal state accordingly.
func (mb *MetricsBuilder) Reset(options ...metricBuilderOption) {
	mb.startTime = pdata.NewTimestampFromTime(time.Now())
	for _, op := range options {
		op(mb)
	}
}

func (mb *MetricsBuilder) Record(metricName string, ts pdata.Timestamp, value interface{}, attributes ...string) error {
	switch metricName {

	case "elasticsearch.cluster.data_nodes":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordElasticsearchClusterDataNodesDataPoint(ts, intVal)
	case "elasticsearch.cluster.health":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordElasticsearchClusterHealthDataPoint(ts, intVal, attributes[0])
	case "elasticsearch.cluster.nodes":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordElasticsearchClusterNodesDataPoint(ts, intVal)
	case "elasticsearch.cluster.shards":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordElasticsearchClusterShardsDataPoint(ts, intVal, attributes[0])
	case "elasticsearch.node.cache.evictions":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordElasticsearchNodeCacheEvictionsDataPoint(ts, intVal, attributes[0])
	case "elasticsearch.node.cache.memory.usage":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordElasticsearchNodeCacheMemoryUsageDataPoint(ts, intVal, attributes[0])
	case "elasticsearch.node.cluster.connections":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordElasticsearchNodeClusterConnectionsDataPoint(ts, intVal)
	case "elasticsearch.node.cluster.io":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordElasticsearchNodeClusterIoDataPoint(ts, intVal, attributes[0])
	case "elasticsearch.node.documents":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordElasticsearchNodeDocumentsDataPoint(ts, intVal, attributes[0])
	case "elasticsearch.node.fs.disk.available":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordElasticsearchNodeFsDiskAvailableDataPoint(ts, intVal)
	case "elasticsearch.node.http.connections":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordElasticsearchNodeHTTPConnectionsDataPoint(ts, intVal)
	case "elasticsearch.node.open_files":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordElasticsearchNodeOpenFilesDataPoint(ts, intVal)
	case "elasticsearch.node.operations.completed":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordElasticsearchNodeOperationsCompletedDataPoint(ts, intVal, attributes[0])
	case "elasticsearch.node.operations.time":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordElasticsearchNodeOperationsTimeDataPoint(ts, intVal, attributes[0])
	case "elasticsearch.node.shards.size":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordElasticsearchNodeShardsSizeDataPoint(ts, intVal)
	case "elasticsearch.node.thread_pool.tasks.finished":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordElasticsearchNodeThreadPoolTasksFinishedDataPoint(ts, intVal, attributes[0], attributes[1])
	case "elasticsearch.node.thread_pool.tasks.queued":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordElasticsearchNodeThreadPoolTasksQueuedDataPoint(ts, intVal, attributes[0])
	case "elasticsearch.node.thread_pool.threads":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordElasticsearchNodeThreadPoolThreadsDataPoint(ts, intVal, attributes[0], attributes[1])
	case "jvm.classes.loaded":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordJvmClassesLoadedDataPoint(ts, intVal)
	case "jvm.gc.collections.count":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordJvmGcCollectionsCountDataPoint(ts, intVal, attributes[0])
	case "jvm.gc.collections.elapsed":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordJvmGcCollectionsElapsedDataPoint(ts, intVal, attributes[0])
	case "jvm.memory.heap.committed":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordJvmMemoryHeapCommittedDataPoint(ts, intVal)
	case "jvm.memory.heap.max":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordJvmMemoryHeapMaxDataPoint(ts, intVal)
	case "jvm.memory.heap.used":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordJvmMemoryHeapUsedDataPoint(ts, intVal)
	case "jvm.memory.nonheap.committed":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordJvmMemoryNonheapCommittedDataPoint(ts, intVal)
	case "jvm.memory.nonheap.used":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordJvmMemoryNonheapUsedDataPoint(ts, intVal)
	case "jvm.memory.pool.max":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordJvmMemoryPoolMaxDataPoint(ts, intVal, attributes[0])
	case "jvm.memory.pool.used":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordJvmMemoryPoolUsedDataPoint(ts, intVal, attributes[0])
	case "jvm.threads.count":
		intVal, ok := value.(int64)
		if !ok {
			return fmt.Errorf("invalid data point value")
		}
		mb.RecordJvmThreadsCountDataPoint(ts, intVal)
	}
	return nil
}

// Attributes contains the possible metric attributes that can be used.
var Attributes = struct {
	// CacheName (The name of cache.)
	CacheName string
	// CollectorName (The name of the garbage collector.)
	CollectorName string
	// Direction (The direction of network data.)
	Direction string
	// DiskUsageState (The state of a section of space on disk.)
	DiskUsageState string
	// DocumentState (The state of the document.)
	DocumentState string
	// ElasticsearchClusterName (The name of the elasticsearch cluster.)
	ElasticsearchClusterName string
	// ElasticsearchNodeName (The name of the elasticsearch node.)
	ElasticsearchNodeName string
	// FsDirection (The direction of filesystem IO.)
	FsDirection string
	// HealthStatus (The health status of the cluster.)
	HealthStatus string
	// MemoryPoolName (The name of the JVM memory pool.)
	MemoryPoolName string
	// Operation (The type of operation.)
	Operation string
	// ShardState (The state of the shard.)
	ShardState string
	// TaskState (The state of the task.)
	TaskState string
	// ThreadPoolName (The name of the thread pool.)
	ThreadPoolName string
	// ThreadState (The state of the thread.)
	ThreadState string
}{
	"cache_name",
	"name",
	"direction",
	"state",
	"state",
	"elasticsearch.cluster.name",
	"elasticsearch.node.name",
	"direction",
	"status",
	"name",
	"operation",
	"state",
	"state",
	"thread_pool_name",
	"state",
}

var metricsByName = map[string]MetricIntf{
	"elasticsearch.cluster.data_nodes":              MetricMetadataElasticsearchClusterDataNodes{},
	"elasticsearch.cluster.health":                  MetricMetadataElasticsearchClusterHealth{},
	"elasticsearch.cluster.nodes":                   MetricMetadataElasticsearchClusterNodes{},
	"elasticsearch.cluster.shards":                  MetricMetadataElasticsearchClusterShards{},
	"elasticsearch.node.cache.evictions":            MetricMetadataElasticsearchNodeCacheEvictions{},
	"elasticsearch.node.cache.memory.usage":         MetricMetadataElasticsearchNodeCacheMemoryUsage{},
	"elasticsearch.node.cluster.connections":        MetricMetadataElasticsearchNodeClusterConnections{},
	"elasticsearch.node.cluster.io":                 MetricMetadataElasticsearchNodeClusterIo{},
	"elasticsearch.node.documents":                  MetricMetadataElasticsearchNodeDocuments{},
	"elasticsearch.node.fs.disk.available":          MetricMetadataElasticsearchNodeFsDiskAvailable{},
	"elasticsearch.node.http.connections":           MetricMetadataElasticsearchNodeHTTPConnections{},
	"elasticsearch.node.open_files":                 MetricMetadataElasticsearchNodeOpenFiles{},
	"elasticsearch.node.operations.completed":       MetricMetadataElasticsearchNodeOperationsCompleted{},
	"elasticsearch.node.operations.time":            MetricMetadataElasticsearchNodeOperationsTime{},
	"elasticsearch.node.shards.size":                MetricMetadataElasticsearchNodeShardsSize{},
	"elasticsearch.node.thread_pool.tasks.finished": MetricMetadataElasticsearchNodeThreadPoolTasksFinished{},
	"elasticsearch.node.thread_pool.tasks.queued":   MetricMetadataElasticsearchNodeThreadPoolTasksQueued{},
	"elasticsearch.node.thread_pool.threads":        MetricMetadataElasticsearchNodeThreadPoolThreads{},
	"jvm.classes.loaded":                            MetricMetadataJvmClassesLoaded{},
	"jvm.gc.collections.count":                      MetricMetadataJvmGcCollectionsCount{},
	"jvm.gc.collections.elapsed":                    MetricMetadataJvmGcCollectionsElapsed{},
	"jvm.memory.heap.committed":                     MetricMetadataJvmMemoryHeapCommitted{},
	"jvm.memory.heap.max":                           MetricMetadataJvmMemoryHeapMax{},
	"jvm.memory.heap.used":                          MetricMetadataJvmMemoryHeapUsed{},
	"jvm.memory.nonheap.committed":                  MetricMetadataJvmMemoryNonheapCommitted{},
	"jvm.memory.nonheap.used":                       MetricMetadataJvmMemoryNonheapUsed{},
	"jvm.memory.pool.max":                           MetricMetadataJvmMemoryPoolMax{},
	"jvm.memory.pool.used":                          MetricMetadataJvmMemoryPoolUsed{},
	"jvm.threads.count":                             MetricMetadataJvmThreadsCount{},
}

func EnabledMetrics(settings MetricsSettings) map[string]bool {
	return map[string]bool{
		"elasticsearch.cluster.data_nodes":              settings.ElasticsearchClusterDataNodes.Enabled,
		"elasticsearch.cluster.health":                  settings.ElasticsearchClusterHealth.Enabled,
		"elasticsearch.cluster.nodes":                   settings.ElasticsearchClusterNodes.Enabled,
		"elasticsearch.cluster.shards":                  settings.ElasticsearchClusterShards.Enabled,
		"elasticsearch.node.cache.evictions":            settings.ElasticsearchNodeCacheEvictions.Enabled,
		"elasticsearch.node.cache.memory.usage":         settings.ElasticsearchNodeCacheMemoryUsage.Enabled,
		"elasticsearch.node.cluster.connections":        settings.ElasticsearchNodeClusterConnections.Enabled,
		"elasticsearch.node.cluster.io":                 settings.ElasticsearchNodeClusterIo.Enabled,
		"elasticsearch.node.documents":                  settings.ElasticsearchNodeDocuments.Enabled,
		"elasticsearch.node.fs.disk.available":          settings.ElasticsearchNodeFsDiskAvailable.Enabled,
		"elasticsearch.node.http.connections":           settings.ElasticsearchNodeHTTPConnections.Enabled,
		"elasticsearch.node.open_files":                 settings.ElasticsearchNodeOpenFiles.Enabled,
		"elasticsearch.node.operations.completed":       settings.ElasticsearchNodeOperationsCompleted.Enabled,
		"elasticsearch.node.operations.time":            settings.ElasticsearchNodeOperationsTime.Enabled,
		"elasticsearch.node.shards.size":                settings.ElasticsearchNodeShardsSize.Enabled,
		"elasticsearch.node.thread_pool.tasks.finished": settings.ElasticsearchNodeThreadPoolTasksFinished.Enabled,
		"elasticsearch.node.thread_pool.tasks.queued":   settings.ElasticsearchNodeThreadPoolTasksQueued.Enabled,
		"elasticsearch.node.thread_pool.threads":        settings.ElasticsearchNodeThreadPoolThreads.Enabled,
		"jvm.classes.loaded":                            settings.JvmClassesLoaded.Enabled,
		"jvm.gc.collections.count":                      settings.JvmGcCollectionsCount.Enabled,
		"jvm.gc.collections.elapsed":                    settings.JvmGcCollectionsElapsed.Enabled,
		"jvm.memory.heap.committed":                     settings.JvmMemoryHeapCommitted.Enabled,
		"jvm.memory.heap.max":                           settings.JvmMemoryHeapMax.Enabled,
		"jvm.memory.heap.used":                          settings.JvmMemoryHeapUsed.Enabled,
		"jvm.memory.nonheap.committed":                  settings.JvmMemoryNonheapCommitted.Enabled,
		"jvm.memory.nonheap.used":                       settings.JvmMemoryNonheapUsed.Enabled,
		"jvm.memory.pool.max":                           settings.JvmMemoryPoolMax.Enabled,
		"jvm.memory.pool.used":                          settings.JvmMemoryPoolUsed.Enabled,
		"jvm.threads.count":                             settings.JvmThreadsCount.Enabled,
	}
}

func ByName(n string) MetricIntf {
	return metricsByName[n]
}

// A is an alias for Attributes.
var A = Attributes

// AttributeCacheName are the possible values that the attribute "cache_name" can have.
var AttributeCacheName = struct {
	Fielddata string
	Query     string
}{
	"fielddata",
	"query",
}

// AttributeDirection are the possible values that the attribute "direction" can have.
var AttributeDirection = struct {
	Received string
	Sent     string
}{
	"received",
	"sent",
}

// AttributeDiskUsageState are the possible values that the attribute "disk_usage_state" can have.
var AttributeDiskUsageState = struct {
	Used string
	Free string
}{
	"used",
	"free",
}

// AttributeDocumentState are the possible values that the attribute "document_state" can have.
var AttributeDocumentState = struct {
	Active  string
	Deleted string
}{
	"active",
	"deleted",
}

// AttributeFsDirection are the possible values that the attribute "fs_direction" can have.
var AttributeFsDirection = struct {
	Read  string
	Write string
}{
	"read",
	"write",
}

// AttributeHealthStatus are the possible values that the attribute "health_status" can have.
var AttributeHealthStatus = struct {
	Green  string
	Yellow string
	Red    string
}{
	"green",
	"yellow",
	"red",
}

// AttributeOperation are the possible values that the attribute "operation" can have.
var AttributeOperation = struct {
	Index   string
	Delete  string
	Get     string
	Query   string
	Fetch   string
	Scroll  string
	Suggest string
	Merge   string
	Refresh string
	Flush   string
	Warmer  string
}{
	"index",
	"delete",
	"get",
	"query",
	"fetch",
	"scroll",
	"suggest",
	"merge",
	"refresh",
	"flush",
	"warmer",
}

// AttributeShardState are the possible values that the attribute "shard_state" can have.
var AttributeShardState = struct {
	Active       string
	Relocating   string
	Initializing string
	Unassigned   string
}{
	"active",
	"relocating",
	"initializing",
	"unassigned",
}

// AttributeTaskState are the possible values that the attribute "task_state" can have.
var AttributeTaskState = struct {
	Rejected  string
	Completed string
}{
	"rejected",
	"completed",
}

// AttributeThreadState are the possible values that the attribute "thread_state" can have.
var AttributeThreadState = struct {
	Active string
	Idle   string
}{
	"active",
	"idle",
}
