// Code generated by mdatagen. DO NOT EDIT.

package metadata

import (
	"time"

	"go.opentelemetry.io/collector/component"
	"go.opentelemetry.io/collector/pdata/pcommon"
	"go.opentelemetry.io/collector/pdata/pmetric"
)

// MetricSettings provides common settings for a particular metric.
type MetricSettings struct {
	Enabled bool `mapstructure:"enabled"`
}

// MetricsSettings provides settings for postgresqlreceiver metrics.
type MetricsSettings struct {
	PostgresqlBackends                 MetricSettings `mapstructure:"postgresql.backends"`
	PostgresqlBgwriterBuffersAllocated MetricSettings `mapstructure:"postgresql.bgwriter.buffers.allocated"`
	PostgresqlBgwriterBuffersWrites    MetricSettings `mapstructure:"postgresql.bgwriter.buffers.writes"`
	PostgresqlBgwriterCheckpointCount  MetricSettings `mapstructure:"postgresql.bgwriter.checkpoint.count"`
	PostgresqlBgwriterDuration         MetricSettings `mapstructure:"postgresql.bgwriter.duration"`
	PostgresqlBgwriterMaxwrittenCount  MetricSettings `mapstructure:"postgresql.bgwriter.maxwritten.count"`
	PostgresqlBlockCount               MetricSettings `mapstructure:"postgresql.block.count"`
	PostgresqlBlocksRead               MetricSettings `mapstructure:"postgresql.blocks_read"`
	PostgresqlCommits                  MetricSettings `mapstructure:"postgresql.commits"`
	PostgresqlDatabaseTableIndexScans  MetricSettings `mapstructure:"postgresql.database.table.index.scans"`
	PostgresqlDatabaseTableIndexSize   MetricSettings `mapstructure:"postgresql.database.table.index.size"`
	PostgresqlDatabaseTransactions     MetricSettings `mapstructure:"postgresql.database.transactions"`
	PostgresqlDbSize                   MetricSettings `mapstructure:"postgresql.db_size"`
	PostgresqlOperations               MetricSettings `mapstructure:"postgresql.operations"`
	PostgresqlQueryBlockCount          MetricSettings `mapstructure:"postgresql.query.block.count"`
	PostgresqlQueryCount               MetricSettings `mapstructure:"postgresql.query.count"`
	PostgresqlQueryDurationAverage     MetricSettings `mapstructure:"postgresql.query.duration.average"`
	PostgresqlQueryDurationTotal       MetricSettings `mapstructure:"postgresql.query.duration.total"`
	PostgresqlReplicationDataDelay     MetricSettings `mapstructure:"postgresql.replication.data_delay"`
	PostgresqlReplicationDelay         MetricSettings `mapstructure:"postgresql.replication.delay"`
	PostgresqlRollbacks                MetricSettings `mapstructure:"postgresql.rollbacks"`
	PostgresqlRows                     MetricSettings `mapstructure:"postgresql.rows"`
	PostgresqlTableCount               MetricSettings `mapstructure:"postgresql.table.count"`
}

func DefaultMetricsSettings() MetricsSettings {
	return MetricsSettings{
		PostgresqlBackends: MetricSettings{
			Enabled: true,
		},
		PostgresqlBgwriterBuffersAllocated: MetricSettings{
			Enabled: true,
		},
		PostgresqlBgwriterBuffersWrites: MetricSettings{
			Enabled: true,
		},
		PostgresqlBgwriterCheckpointCount: MetricSettings{
			Enabled: true,
		},
		PostgresqlBgwriterDuration: MetricSettings{
			Enabled: true,
		},
		PostgresqlBgwriterMaxwrittenCount: MetricSettings{
			Enabled: true,
		},
		PostgresqlBlockCount: MetricSettings{
			Enabled: true,
		},
		PostgresqlBlocksRead: MetricSettings{
			Enabled: true,
		},
		PostgresqlCommits: MetricSettings{
			Enabled: true,
		},
		PostgresqlDatabaseTableIndexScans: MetricSettings{
			Enabled: true,
		},
		PostgresqlDatabaseTableIndexSize: MetricSettings{
			Enabled: true,
		},
		PostgresqlDatabaseTransactions: MetricSettings{
			Enabled: true,
		},
		PostgresqlDbSize: MetricSettings{
			Enabled: true,
		},
		PostgresqlOperations: MetricSettings{
			Enabled: true,
		},
		PostgresqlQueryBlockCount: MetricSettings{
			Enabled: true,
		},
		PostgresqlQueryCount: MetricSettings{
			Enabled: true,
		},
		PostgresqlQueryDurationAverage: MetricSettings{
			Enabled: true,
		},
		PostgresqlQueryDurationTotal: MetricSettings{
			Enabled: true,
		},
		PostgresqlReplicationDataDelay: MetricSettings{
			Enabled: true,
		},
		PostgresqlReplicationDelay: MetricSettings{
			Enabled: true,
		},
		PostgresqlRollbacks: MetricSettings{
			Enabled: true,
		},
		PostgresqlRows: MetricSettings{
			Enabled: true,
		},
		PostgresqlTableCount: MetricSettings{
			Enabled: true,
		},
	}
}

// AttributeBgBufferSource specifies the a value bg_buffer_source attribute.
type AttributeBgBufferSource int

const (
	_ AttributeBgBufferSource = iota
	AttributeBgBufferSourceBackend
	AttributeBgBufferSourceBackendFsync
	AttributeBgBufferSourceCheckpoints
	AttributeBgBufferSourceBgwriter
)

// String returns the string representation of the AttributeBgBufferSource.
func (av AttributeBgBufferSource) String() string {
	switch av {
	case AttributeBgBufferSourceBackend:
		return "backend"
	case AttributeBgBufferSourceBackendFsync:
		return "backend_fsync"
	case AttributeBgBufferSourceCheckpoints:
		return "checkpoints"
	case AttributeBgBufferSourceBgwriter:
		return "bgwriter"
	}
	return ""
}

// MapAttributeBgBufferSource is a helper map of string to AttributeBgBufferSource attribute value.
var MapAttributeBgBufferSource = map[string]AttributeBgBufferSource{
	"backend":       AttributeBgBufferSourceBackend,
	"backend_fsync": AttributeBgBufferSourceBackendFsync,
	"checkpoints":   AttributeBgBufferSourceCheckpoints,
	"bgwriter":      AttributeBgBufferSourceBgwriter,
}

// AttributeBgCheckpointType specifies the a value bg_checkpoint_type attribute.
type AttributeBgCheckpointType int

const (
	_ AttributeBgCheckpointType = iota
	AttributeBgCheckpointTypeRequested
	AttributeBgCheckpointTypeScheduled
)

// String returns the string representation of the AttributeBgCheckpointType.
func (av AttributeBgCheckpointType) String() string {
	switch av {
	case AttributeBgCheckpointTypeRequested:
		return "requested"
	case AttributeBgCheckpointTypeScheduled:
		return "scheduled"
	}
	return ""
}

// MapAttributeBgCheckpointType is a helper map of string to AttributeBgCheckpointType attribute value.
var MapAttributeBgCheckpointType = map[string]AttributeBgCheckpointType{
	"requested": AttributeBgCheckpointTypeRequested,
	"scheduled": AttributeBgCheckpointTypeScheduled,
}

// AttributeBgDurationType specifies the a value bg_duration_type attribute.
type AttributeBgDurationType int

const (
	_ AttributeBgDurationType = iota
	AttributeBgDurationTypeSync
	AttributeBgDurationTypeWrite
)

// String returns the string representation of the AttributeBgDurationType.
func (av AttributeBgDurationType) String() string {
	switch av {
	case AttributeBgDurationTypeSync:
		return "sync"
	case AttributeBgDurationTypeWrite:
		return "write"
	}
	return ""
}

// MapAttributeBgDurationType is a helper map of string to AttributeBgDurationType attribute value.
var MapAttributeBgDurationType = map[string]AttributeBgDurationType{
	"sync":  AttributeBgDurationTypeSync,
	"write": AttributeBgDurationTypeWrite,
}

// AttributeOperation specifies the a value operation attribute.
type AttributeOperation int

const (
	_ AttributeOperation = iota
	AttributeOperationIns
	AttributeOperationUpd
	AttributeOperationDel
	AttributeOperationHotUpd
)

// String returns the string representation of the AttributeOperation.
func (av AttributeOperation) String() string {
	switch av {
	case AttributeOperationIns:
		return "ins"
	case AttributeOperationUpd:
		return "upd"
	case AttributeOperationDel:
		return "del"
	case AttributeOperationHotUpd:
		return "hot_upd"
	}
	return ""
}

// MapAttributeOperation is a helper map of string to AttributeOperation attribute value.
var MapAttributeOperation = map[string]AttributeOperation{
	"ins":     AttributeOperationIns,
	"upd":     AttributeOperationUpd,
	"del":     AttributeOperationDel,
	"hot_upd": AttributeOperationHotUpd,
}

// AttributeQueryBlockOperation specifies the a value query_block_operation attribute.
type AttributeQueryBlockOperation int

const (
	_ AttributeQueryBlockOperation = iota
	AttributeQueryBlockOperationDirty
	AttributeQueryBlockOperationRead
	AttributeQueryBlockOperationWrite
)

// String returns the string representation of the AttributeQueryBlockOperation.
func (av AttributeQueryBlockOperation) String() string {
	switch av {
	case AttributeQueryBlockOperationDirty:
		return "dirty"
	case AttributeQueryBlockOperationRead:
		return "read"
	case AttributeQueryBlockOperationWrite:
		return "write"
	}
	return ""
}

// MapAttributeQueryBlockOperation is a helper map of string to AttributeQueryBlockOperation attribute value.
var MapAttributeQueryBlockOperation = map[string]AttributeQueryBlockOperation{
	"dirty": AttributeQueryBlockOperationDirty,
	"read":  AttributeQueryBlockOperationRead,
	"write": AttributeQueryBlockOperationWrite,
}

// AttributeQueryBlockType specifies the a value query_block_type attribute.
type AttributeQueryBlockType int

const (
	_ AttributeQueryBlockType = iota
	AttributeQueryBlockTypeLocal
	AttributeQueryBlockTypeShared
	AttributeQueryBlockTypeTemp
)

// String returns the string representation of the AttributeQueryBlockType.
func (av AttributeQueryBlockType) String() string {
	switch av {
	case AttributeQueryBlockTypeLocal:
		return "local"
	case AttributeQueryBlockTypeShared:
		return "shared"
	case AttributeQueryBlockTypeTemp:
		return "temp"
	}
	return ""
}

// MapAttributeQueryBlockType is a helper map of string to AttributeQueryBlockType attribute value.
var MapAttributeQueryBlockType = map[string]AttributeQueryBlockType{
	"local":  AttributeQueryBlockTypeLocal,
	"shared": AttributeQueryBlockTypeShared,
	"temp":   AttributeQueryBlockTypeTemp,
}

// AttributeSource specifies the a value source attribute.
type AttributeSource int

const (
	_ AttributeSource = iota
	AttributeSourceHeapRead
	AttributeSourceHeapHit
	AttributeSourceIdxRead
	AttributeSourceIdxHit
	AttributeSourceToastRead
	AttributeSourceToastHit
	AttributeSourceTidxRead
	AttributeSourceTidxHit
)

// String returns the string representation of the AttributeSource.
func (av AttributeSource) String() string {
	switch av {
	case AttributeSourceHeapRead:
		return "heap_read"
	case AttributeSourceHeapHit:
		return "heap_hit"
	case AttributeSourceIdxRead:
		return "idx_read"
	case AttributeSourceIdxHit:
		return "idx_hit"
	case AttributeSourceToastRead:
		return "toast_read"
	case AttributeSourceToastHit:
		return "toast_hit"
	case AttributeSourceTidxRead:
		return "tidx_read"
	case AttributeSourceTidxHit:
		return "tidx_hit"
	}
	return ""
}

// MapAttributeSource is a helper map of string to AttributeSource attribute value.
var MapAttributeSource = map[string]AttributeSource{
	"heap_read":  AttributeSourceHeapRead,
	"heap_hit":   AttributeSourceHeapHit,
	"idx_read":   AttributeSourceIdxRead,
	"idx_hit":    AttributeSourceIdxHit,
	"toast_read": AttributeSourceToastRead,
	"toast_hit":  AttributeSourceToastHit,
	"tidx_read":  AttributeSourceTidxRead,
	"tidx_hit":   AttributeSourceTidxHit,
}

// AttributeState specifies the a value state attribute.
type AttributeState int

const (
	_ AttributeState = iota
	AttributeStateDead
	AttributeStateLive
)

// String returns the string representation of the AttributeState.
func (av AttributeState) String() string {
	switch av {
	case AttributeStateDead:
		return "dead"
	case AttributeStateLive:
		return "live"
	}
	return ""
}

// MapAttributeState is a helper map of string to AttributeState attribute value.
var MapAttributeState = map[string]AttributeState{
	"dead": AttributeStateDead,
	"live": AttributeStateLive,
}

// AttributeTransactionResult specifies the a value transaction_result attribute.
type AttributeTransactionResult int

const (
	_ AttributeTransactionResult = iota
	AttributeTransactionResultRollback
	AttributeTransactionResultCommitted
)

// String returns the string representation of the AttributeTransactionResult.
func (av AttributeTransactionResult) String() string {
	switch av {
	case AttributeTransactionResultRollback:
		return "rollback"
	case AttributeTransactionResultCommitted:
		return "committed"
	}
	return ""
}

// MapAttributeTransactionResult is a helper map of string to AttributeTransactionResult attribute value.
var MapAttributeTransactionResult = map[string]AttributeTransactionResult{
	"rollback":  AttributeTransactionResultRollback,
	"committed": AttributeTransactionResultCommitted,
}

type metricPostgresqlBackends struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.backends metric with initial data.
func (m *metricPostgresqlBackends) init() {
	m.data.SetName("postgresql.backends")
	m.data.SetDescription("The number of backends.")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBackends) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert("database", pcommon.NewValueString(databaseAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBackends) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBackends) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBackends(settings MetricSettings) metricPostgresqlBackends {
	m := metricPostgresqlBackends{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBgwriterBuffersAllocated struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.bgwriter.buffers.allocated metric with initial data.
func (m *metricPostgresqlBgwriterBuffersAllocated) init() {
	m.data.SetName("postgresql.bgwriter.buffers.allocated")
	m.data.SetDescription("Number of buffers allocated")
	m.data.SetUnit("{buffers}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricPostgresqlBgwriterBuffersAllocated) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBgwriterBuffersAllocated) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBgwriterBuffersAllocated) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBgwriterBuffersAllocated(settings MetricSettings) metricPostgresqlBgwriterBuffersAllocated {
	m := metricPostgresqlBgwriterBuffersAllocated{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBgwriterBuffersWrites struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.bgwriter.buffers.writes metric with initial data.
func (m *metricPostgresqlBgwriterBuffersWrites) init() {
	m.data.SetName("postgresql.bgwriter.buffers.writes")
	m.data.SetDescription("Number of buffers written")
	m.data.SetUnit("{buffers}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBgwriterBuffersWrites) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, bgBufferSourceAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert("source", pcommon.NewValueString(bgBufferSourceAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBgwriterBuffersWrites) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBgwriterBuffersWrites) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBgwriterBuffersWrites(settings MetricSettings) metricPostgresqlBgwriterBuffersWrites {
	m := metricPostgresqlBgwriterBuffersWrites{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBgwriterCheckpointCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.bgwriter.checkpoint.count metric with initial data.
func (m *metricPostgresqlBgwriterCheckpointCount) init() {
	m.data.SetName("postgresql.bgwriter.checkpoint.count")
	m.data.SetDescription("The number of checkpoints performed")
	m.data.SetUnit("{checkpoints}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBgwriterCheckpointCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, bgCheckpointTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert("type", pcommon.NewValueString(bgCheckpointTypeAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBgwriterCheckpointCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBgwriterCheckpointCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBgwriterCheckpointCount(settings MetricSettings) metricPostgresqlBgwriterCheckpointCount {
	m := metricPostgresqlBgwriterCheckpointCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBgwriterDuration struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.bgwriter.duration metric with initial data.
func (m *metricPostgresqlBgwriterDuration) init() {
	m.data.SetName("postgresql.bgwriter.duration")
	m.data.SetDescription("Total time spent writing and syncing files to disk by checkpoints.")
	m.data.SetUnit("ms")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBgwriterDuration) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, bgDurationTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert("type", pcommon.NewValueString(bgDurationTypeAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBgwriterDuration) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBgwriterDuration) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBgwriterDuration(settings MetricSettings) metricPostgresqlBgwriterDuration {
	m := metricPostgresqlBgwriterDuration{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBgwriterMaxwrittenCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.bgwriter.maxwritten.count metric with initial data.
func (m *metricPostgresqlBgwriterMaxwrittenCount) init() {
	m.data.SetName("postgresql.bgwriter.maxwritten.count")
	m.data.SetDescription("Number of times the background writer stopped a cleaning scan because it had written too many buffers.")
	m.data.SetUnit("")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricPostgresqlBgwriterMaxwrittenCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBgwriterMaxwrittenCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBgwriterMaxwrittenCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBgwriterMaxwrittenCount(settings MetricSettings) metricPostgresqlBgwriterMaxwrittenCount {
	m := metricPostgresqlBgwriterMaxwrittenCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBlockCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.block.count metric with initial data.
func (m *metricPostgresqlBlockCount) init() {
	m.data.SetName("postgresql.block.count")
	m.data.SetDescription("Total number of blocks")
	m.data.SetUnit("{blocks}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricPostgresqlBlockCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBlockCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBlockCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBlockCount(settings MetricSettings) metricPostgresqlBlockCount {
	m := metricPostgresqlBlockCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBlocksRead struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.blocks_read metric with initial data.
func (m *metricPostgresqlBlocksRead) init() {
	m.data.SetName("postgresql.blocks_read")
	m.data.SetDescription("The number of blocks read.")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBlocksRead) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseAttributeValue string, tableAttributeValue string, sourceAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert("database", pcommon.NewValueString(databaseAttributeValue))
	dp.Attributes().Insert("table", pcommon.NewValueString(tableAttributeValue))
	dp.Attributes().Insert("source", pcommon.NewValueString(sourceAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBlocksRead) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBlocksRead) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBlocksRead(settings MetricSettings) metricPostgresqlBlocksRead {
	m := metricPostgresqlBlocksRead{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlCommits struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.commits metric with initial data.
func (m *metricPostgresqlCommits) init() {
	m.data.SetName("postgresql.commits")
	m.data.SetDescription("The number of commits.")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlCommits) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert("database", pcommon.NewValueString(databaseAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlCommits) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlCommits) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlCommits(settings MetricSettings) metricPostgresqlCommits {
	m := metricPostgresqlCommits{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlDatabaseTableIndexScans struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.database.table.index.scans metric with initial data.
func (m *metricPostgresqlDatabaseTableIndexScans) init() {
	m.data.SetName("postgresql.database.table.index.scans")
	m.data.SetDescription("The number of index scans on a table.")
	m.data.SetUnit("{scans}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricPostgresqlDatabaseTableIndexScans) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlDatabaseTableIndexScans) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlDatabaseTableIndexScans) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlDatabaseTableIndexScans(settings MetricSettings) metricPostgresqlDatabaseTableIndexScans {
	m := metricPostgresqlDatabaseTableIndexScans{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlDatabaseTableIndexSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.database.table.index.size metric with initial data.
func (m *metricPostgresqlDatabaseTableIndexSize) init() {
	m.data.SetName("postgresql.database.table.index.size")
	m.data.SetDescription("Size of the index on disk.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricPostgresqlDatabaseTableIndexSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlDatabaseTableIndexSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlDatabaseTableIndexSize) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlDatabaseTableIndexSize(settings MetricSettings) metricPostgresqlDatabaseTableIndexSize {
	m := metricPostgresqlDatabaseTableIndexSize{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlDatabaseTransactions struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.database.transactions metric with initial data.
func (m *metricPostgresqlDatabaseTransactions) init() {
	m.data.SetName("postgresql.database.transactions")
	m.data.SetDescription("Number of transactions rolled back or committed.")
	m.data.SetUnit("{transactions}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlDatabaseTransactions) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, transactionResultAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert("result", pcommon.NewValueString(transactionResultAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlDatabaseTransactions) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlDatabaseTransactions) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlDatabaseTransactions(settings MetricSettings) metricPostgresqlDatabaseTransactions {
	m := metricPostgresqlDatabaseTransactions{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlDbSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.db_size metric with initial data.
func (m *metricPostgresqlDbSize) init() {
	m.data.SetName("postgresql.db_size")
	m.data.SetDescription("The database disk usage.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlDbSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert("database", pcommon.NewValueString(databaseAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlDbSize) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlDbSize) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlDbSize(settings MetricSettings) metricPostgresqlDbSize {
	m := metricPostgresqlDbSize{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlOperations struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.operations metric with initial data.
func (m *metricPostgresqlOperations) init() {
	m.data.SetName("postgresql.operations")
	m.data.SetDescription("The number of db row operations.")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlOperations) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseAttributeValue string, tableAttributeValue string, operationAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert("database", pcommon.NewValueString(databaseAttributeValue))
	dp.Attributes().Insert("table", pcommon.NewValueString(tableAttributeValue))
	dp.Attributes().Insert("operation", pcommon.NewValueString(operationAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlOperations) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlOperations) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlOperations(settings MetricSettings) metricPostgresqlOperations {
	m := metricPostgresqlOperations{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlQueryBlockCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.query.block.count metric with initial data.
func (m *metricPostgresqlQueryBlockCount) init() {
	m.data.SetName("postgresql.query.block.count")
	m.data.SetDescription("Total number of blocks dirtied, read, or written by queries.")
	m.data.SetUnit("{queries}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlQueryBlockCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, queryBlockOperationAttributeValue string, queryBlockTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert("operation", pcommon.NewValueString(queryBlockOperationAttributeValue))
	dp.Attributes().Insert("block", pcommon.NewValueString(queryBlockTypeAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlQueryBlockCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlQueryBlockCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlQueryBlockCount(settings MetricSettings) metricPostgresqlQueryBlockCount {
	m := metricPostgresqlQueryBlockCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlQueryCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.query.count metric with initial data.
func (m *metricPostgresqlQueryCount) init() {
	m.data.SetName("postgresql.query.count")
	m.data.SetDescription("Number of queries executed.")
	m.data.SetUnit("{queries}")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricPostgresqlQueryCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlQueryCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlQueryCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlQueryCount(settings MetricSettings) metricPostgresqlQueryCount {
	m := metricPostgresqlQueryCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlQueryDurationAverage struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.query.duration.average metric with initial data.
func (m *metricPostgresqlQueryDurationAverage) init() {
	m.data.SetName("postgresql.query.duration.average")
	m.data.SetDescription("Average time spent executing a query.")
	m.data.SetUnit("ms")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricPostgresqlQueryDurationAverage) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlQueryDurationAverage) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlQueryDurationAverage) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlQueryDurationAverage(settings MetricSettings) metricPostgresqlQueryDurationAverage {
	m := metricPostgresqlQueryDurationAverage{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlQueryDurationTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.query.duration.total metric with initial data.
func (m *metricPostgresqlQueryDurationTotal) init() {
	m.data.SetName("postgresql.query.duration.total")
	m.data.SetDescription("Total time spent executing a query.")
	m.data.SetUnit("ms")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
}

func (m *metricPostgresqlQueryDurationTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlQueryDurationTotal) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlQueryDurationTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlQueryDurationTotal(settings MetricSettings) metricPostgresqlQueryDurationTotal {
	m := metricPostgresqlQueryDurationTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationDataDelay struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication.data_delay metric with initial data.
func (m *metricPostgresqlReplicationDataDelay) init() {
	m.data.SetName("postgresql.replication.data_delay")
	m.data.SetDescription("The amount of data delayed in replication.")
	m.data.SetUnit("By")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricPostgresqlReplicationDataDelay) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationDataDelay) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationDataDelay) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationDataDelay(settings MetricSettings) metricPostgresqlReplicationDataDelay {
	m := metricPostgresqlReplicationDataDelay{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationDelay struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication.delay metric with initial data.
func (m *metricPostgresqlReplicationDelay) init() {
	m.data.SetName("postgresql.replication.delay")
	m.data.SetDescription("The amount of time of lag between the current clock and the timestamp of the last WAL record.")
	m.data.SetUnit("ms")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricPostgresqlReplicationDelay) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationDelay) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationDelay) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationDelay(settings MetricSettings) metricPostgresqlReplicationDelay {
	m := metricPostgresqlReplicationDelay{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRollbacks struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.rollbacks metric with initial data.
func (m *metricPostgresqlRollbacks) init() {
	m.data.SetName("postgresql.rollbacks")
	m.data.SetDescription("The number of rollbacks.")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRollbacks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, databaseAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert("database", pcommon.NewValueString(databaseAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRollbacks) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRollbacks) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRollbacks(settings MetricSettings) metricPostgresqlRollbacks {
	m := metricPostgresqlRollbacks{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRows struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.rows metric with initial data.
func (m *metricPostgresqlRows) init() {
	m.data.SetName("postgresql.rows")
	m.data.SetDescription("The number of rows in the database.")
	m.data.SetUnit("1")
	m.data.SetDataType(pmetric.MetricDataTypeSum)
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.MetricAggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRows) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, stateAttributeValue string, databaseAttributeValue string, tableAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert("state", pcommon.NewValueString(stateAttributeValue))
	dp.Attributes().Insert("database", pcommon.NewValueString(databaseAttributeValue))
	dp.Attributes().Insert("table", pcommon.NewValueString(tableAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRows) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRows) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRows(settings MetricSettings) metricPostgresqlRows {
	m := metricPostgresqlRows{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlTableCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.table.count metric with initial data.
func (m *metricPostgresqlTableCount) init() {
	m.data.SetName("postgresql.table.count")
	m.data.SetDescription("Number of tables in a database.")
	m.data.SetUnit("")
	m.data.SetDataType(pmetric.MetricDataTypeGauge)
}

func (m *metricPostgresqlTableCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlTableCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlTableCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlTableCount(settings MetricSettings) metricPostgresqlTableCount {
	m := metricPostgresqlTableCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

// MetricsBuilder provides an interface for scrapers to report metrics while taking care of all the transformations
// required to produce metric representation defined in metadata and user settings.
type MetricsBuilder struct {
	startTime                                pcommon.Timestamp   // start time that will be applied to all recorded data points.
	metricsCapacity                          int                 // maximum observed number of metrics per resource.
	resourceCapacity                         int                 // maximum observed number of resource attributes.
	metricsBuffer                            pmetric.Metrics     // accumulates metrics data before emitting.
	buildInfo                                component.BuildInfo // contains version information
	metricPostgresqlBackends                 metricPostgresqlBackends
	metricPostgresqlBgwriterBuffersAllocated metricPostgresqlBgwriterBuffersAllocated
	metricPostgresqlBgwriterBuffersWrites    metricPostgresqlBgwriterBuffersWrites
	metricPostgresqlBgwriterCheckpointCount  metricPostgresqlBgwriterCheckpointCount
	metricPostgresqlBgwriterDuration         metricPostgresqlBgwriterDuration
	metricPostgresqlBgwriterMaxwrittenCount  metricPostgresqlBgwriterMaxwrittenCount
	metricPostgresqlBlockCount               metricPostgresqlBlockCount
	metricPostgresqlBlocksRead               metricPostgresqlBlocksRead
	metricPostgresqlCommits                  metricPostgresqlCommits
	metricPostgresqlDatabaseTableIndexScans  metricPostgresqlDatabaseTableIndexScans
	metricPostgresqlDatabaseTableIndexSize   metricPostgresqlDatabaseTableIndexSize
	metricPostgresqlDatabaseTransactions     metricPostgresqlDatabaseTransactions
	metricPostgresqlDbSize                   metricPostgresqlDbSize
	metricPostgresqlOperations               metricPostgresqlOperations
	metricPostgresqlQueryBlockCount          metricPostgresqlQueryBlockCount
	metricPostgresqlQueryCount               metricPostgresqlQueryCount
	metricPostgresqlQueryDurationAverage     metricPostgresqlQueryDurationAverage
	metricPostgresqlQueryDurationTotal       metricPostgresqlQueryDurationTotal
	metricPostgresqlReplicationDataDelay     metricPostgresqlReplicationDataDelay
	metricPostgresqlReplicationDelay         metricPostgresqlReplicationDelay
	metricPostgresqlRollbacks                metricPostgresqlRollbacks
	metricPostgresqlRows                     metricPostgresqlRows
	metricPostgresqlTableCount               metricPostgresqlTableCount
}

// metricBuilderOption applies changes to default metrics builder.
type metricBuilderOption func(*MetricsBuilder)

// WithStartTime sets startTime on the metrics builder.
func WithStartTime(startTime pcommon.Timestamp) metricBuilderOption {
	return func(mb *MetricsBuilder) {
		mb.startTime = startTime
	}
}

func NewMetricsBuilder(settings MetricsSettings, buildInfo component.BuildInfo, options ...metricBuilderOption) *MetricsBuilder {
	mb := &MetricsBuilder{
		startTime:                                pcommon.NewTimestampFromTime(time.Now()),
		metricsBuffer:                            pmetric.NewMetrics(),
		buildInfo:                                buildInfo,
		metricPostgresqlBackends:                 newMetricPostgresqlBackends(settings.PostgresqlBackends),
		metricPostgresqlBgwriterBuffersAllocated: newMetricPostgresqlBgwriterBuffersAllocated(settings.PostgresqlBgwriterBuffersAllocated),
		metricPostgresqlBgwriterBuffersWrites:    newMetricPostgresqlBgwriterBuffersWrites(settings.PostgresqlBgwriterBuffersWrites),
		metricPostgresqlBgwriterCheckpointCount:  newMetricPostgresqlBgwriterCheckpointCount(settings.PostgresqlBgwriterCheckpointCount),
		metricPostgresqlBgwriterDuration:         newMetricPostgresqlBgwriterDuration(settings.PostgresqlBgwriterDuration),
		metricPostgresqlBgwriterMaxwrittenCount:  newMetricPostgresqlBgwriterMaxwrittenCount(settings.PostgresqlBgwriterMaxwrittenCount),
		metricPostgresqlBlockCount:               newMetricPostgresqlBlockCount(settings.PostgresqlBlockCount),
		metricPostgresqlBlocksRead:               newMetricPostgresqlBlocksRead(settings.PostgresqlBlocksRead),
		metricPostgresqlCommits:                  newMetricPostgresqlCommits(settings.PostgresqlCommits),
		metricPostgresqlDatabaseTableIndexScans:  newMetricPostgresqlDatabaseTableIndexScans(settings.PostgresqlDatabaseTableIndexScans),
		metricPostgresqlDatabaseTableIndexSize:   newMetricPostgresqlDatabaseTableIndexSize(settings.PostgresqlDatabaseTableIndexSize),
		metricPostgresqlDatabaseTransactions:     newMetricPostgresqlDatabaseTransactions(settings.PostgresqlDatabaseTransactions),
		metricPostgresqlDbSize:                   newMetricPostgresqlDbSize(settings.PostgresqlDbSize),
		metricPostgresqlOperations:               newMetricPostgresqlOperations(settings.PostgresqlOperations),
		metricPostgresqlQueryBlockCount:          newMetricPostgresqlQueryBlockCount(settings.PostgresqlQueryBlockCount),
		metricPostgresqlQueryCount:               newMetricPostgresqlQueryCount(settings.PostgresqlQueryCount),
		metricPostgresqlQueryDurationAverage:     newMetricPostgresqlQueryDurationAverage(settings.PostgresqlQueryDurationAverage),
		metricPostgresqlQueryDurationTotal:       newMetricPostgresqlQueryDurationTotal(settings.PostgresqlQueryDurationTotal),
		metricPostgresqlReplicationDataDelay:     newMetricPostgresqlReplicationDataDelay(settings.PostgresqlReplicationDataDelay),
		metricPostgresqlReplicationDelay:         newMetricPostgresqlReplicationDelay(settings.PostgresqlReplicationDelay),
		metricPostgresqlRollbacks:                newMetricPostgresqlRollbacks(settings.PostgresqlRollbacks),
		metricPostgresqlRows:                     newMetricPostgresqlRows(settings.PostgresqlRows),
		metricPostgresqlTableCount:               newMetricPostgresqlTableCount(settings.PostgresqlTableCount),
	}
	for _, op := range options {
		op(mb)
	}
	return mb
}

// updateCapacity updates max length of metrics and resource attributes that will be used for the slice capacity.
func (mb *MetricsBuilder) updateCapacity(rm pmetric.ResourceMetrics) {
	if mb.metricsCapacity < rm.ScopeMetrics().At(0).Metrics().Len() {
		mb.metricsCapacity = rm.ScopeMetrics().At(0).Metrics().Len()
	}
	if mb.resourceCapacity < rm.Resource().Attributes().Len() {
		mb.resourceCapacity = rm.Resource().Attributes().Len()
	}
}

// ResourceMetricsOption applies changes to provided resource metrics.
type ResourceMetricsOption func(pmetric.ResourceMetrics)

// WithPostgresqlDatabase sets provided value as "postgresql.database" attribute for current resource.
func WithPostgresqlDatabase(val string) ResourceMetricsOption {
	return func(rm pmetric.ResourceMetrics) {
		rm.Resource().Attributes().UpsertString("postgresql.database", val)
	}
}

// WithPostgresqlDatabaseTable sets provided value as "postgresql.database.table" attribute for current resource.
func WithPostgresqlDatabaseTable(val string) ResourceMetricsOption {
	return func(rm pmetric.ResourceMetrics) {
		rm.Resource().Attributes().UpsertString("postgresql.database.table", val)
	}
}

// WithPostgresqlIndexName sets provided value as "postgresql.index.name" attribute for current resource.
func WithPostgresqlIndexName(val string) ResourceMetricsOption {
	return func(rm pmetric.ResourceMetrics) {
		rm.Resource().Attributes().UpsertString("postgresql.index.name", val)
	}
}

// WithPostgresqlQuery sets provided value as "postgresql.query" attribute for current resource.
func WithPostgresqlQuery(val string) ResourceMetricsOption {
	return func(rm pmetric.ResourceMetrics) {
		rm.Resource().Attributes().UpsertString("postgresql.query", val)
	}
}

// WithStartTimeOverride overrides start time for all the resource metrics data points.
// This option should be only used if different start time has to be set on metrics coming from different resources.
func WithStartTimeOverride(start pcommon.Timestamp) ResourceMetricsOption {
	return func(rm pmetric.ResourceMetrics) {
		var dps pmetric.NumberDataPointSlice
		metrics := rm.ScopeMetrics().At(0).Metrics()
		for i := 0; i < metrics.Len(); i++ {
			switch metrics.At(i).DataType() {
			case pmetric.MetricDataTypeGauge:
				dps = metrics.At(i).Gauge().DataPoints()
			case pmetric.MetricDataTypeSum:
				dps = metrics.At(i).Sum().DataPoints()
			}
			for j := 0; j < dps.Len(); j++ {
				dps.At(j).SetStartTimestamp(start)
			}
		}
	}
}

// EmitForResource saves all the generated metrics under a new resource and updates the internal state to be ready for
// recording another set of data points as part of another resource. This function can be helpful when one scraper
// needs to emit metrics from several resources. Otherwise calling this function is not required,
// just `Emit` function can be called instead.
// Resource attributes should be provided as ResourceMetricsOption arguments.
func (mb *MetricsBuilder) EmitForResource(rmo ...ResourceMetricsOption) {
	rm := pmetric.NewResourceMetrics()
	rm.Resource().Attributes().EnsureCapacity(mb.resourceCapacity)
	ils := rm.ScopeMetrics().AppendEmpty()
	ils.Scope().SetName("otelcol/postgresqlreceiver")
	ils.Scope().SetVersion(mb.buildInfo.Version)
	ils.Metrics().EnsureCapacity(mb.metricsCapacity)
	mb.metricPostgresqlBackends.emit(ils.Metrics())
	mb.metricPostgresqlBgwriterBuffersAllocated.emit(ils.Metrics())
	mb.metricPostgresqlBgwriterBuffersWrites.emit(ils.Metrics())
	mb.metricPostgresqlBgwriterCheckpointCount.emit(ils.Metrics())
	mb.metricPostgresqlBgwriterDuration.emit(ils.Metrics())
	mb.metricPostgresqlBgwriterMaxwrittenCount.emit(ils.Metrics())
	mb.metricPostgresqlBlockCount.emit(ils.Metrics())
	mb.metricPostgresqlBlocksRead.emit(ils.Metrics())
	mb.metricPostgresqlCommits.emit(ils.Metrics())
	mb.metricPostgresqlDatabaseTableIndexScans.emit(ils.Metrics())
	mb.metricPostgresqlDatabaseTableIndexSize.emit(ils.Metrics())
	mb.metricPostgresqlDatabaseTransactions.emit(ils.Metrics())
	mb.metricPostgresqlDbSize.emit(ils.Metrics())
	mb.metricPostgresqlOperations.emit(ils.Metrics())
	mb.metricPostgresqlQueryBlockCount.emit(ils.Metrics())
	mb.metricPostgresqlQueryCount.emit(ils.Metrics())
	mb.metricPostgresqlQueryDurationAverage.emit(ils.Metrics())
	mb.metricPostgresqlQueryDurationTotal.emit(ils.Metrics())
	mb.metricPostgresqlReplicationDataDelay.emit(ils.Metrics())
	mb.metricPostgresqlReplicationDelay.emit(ils.Metrics())
	mb.metricPostgresqlRollbacks.emit(ils.Metrics())
	mb.metricPostgresqlRows.emit(ils.Metrics())
	mb.metricPostgresqlTableCount.emit(ils.Metrics())
	for _, op := range rmo {
		op(rm)
	}
	if ils.Metrics().Len() > 0 {
		mb.updateCapacity(rm)
		rm.MoveTo(mb.metricsBuffer.ResourceMetrics().AppendEmpty())
	}
}

// Emit returns all the metrics accumulated by the metrics builder and updates the internal state to be ready for
// recording another set of metrics. This function will be responsible for applying all the transformations required to
// produce metric representation defined in metadata and user settings, e.g. delta or cumulative.
func (mb *MetricsBuilder) Emit(rmo ...ResourceMetricsOption) pmetric.Metrics {
	mb.EmitForResource(rmo...)
	metrics := pmetric.NewMetrics()
	mb.metricsBuffer.MoveTo(metrics)
	return metrics
}

// RecordPostgresqlBackendsDataPoint adds a data point to postgresql.backends metric.
func (mb *MetricsBuilder) RecordPostgresqlBackendsDataPoint(ts pcommon.Timestamp, val int64, databaseAttributeValue string) {
	mb.metricPostgresqlBackends.recordDataPoint(mb.startTime, ts, val, databaseAttributeValue)
}

// RecordPostgresqlBgwriterBuffersAllocatedDataPoint adds a data point to postgresql.bgwriter.buffers.allocated metric.
func (mb *MetricsBuilder) RecordPostgresqlBgwriterBuffersAllocatedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricPostgresqlBgwriterBuffersAllocated.recordDataPoint(mb.startTime, ts, val)
}

// RecordPostgresqlBgwriterBuffersWritesDataPoint adds a data point to postgresql.bgwriter.buffers.writes metric.
func (mb *MetricsBuilder) RecordPostgresqlBgwriterBuffersWritesDataPoint(ts pcommon.Timestamp, val int64, bgBufferSourceAttributeValue AttributeBgBufferSource) {
	mb.metricPostgresqlBgwriterBuffersWrites.recordDataPoint(mb.startTime, ts, val, bgBufferSourceAttributeValue.String())
}

// RecordPostgresqlBgwriterCheckpointCountDataPoint adds a data point to postgresql.bgwriter.checkpoint.count metric.
func (mb *MetricsBuilder) RecordPostgresqlBgwriterCheckpointCountDataPoint(ts pcommon.Timestamp, val int64, bgCheckpointTypeAttributeValue AttributeBgCheckpointType) {
	mb.metricPostgresqlBgwriterCheckpointCount.recordDataPoint(mb.startTime, ts, val, bgCheckpointTypeAttributeValue.String())
}

// RecordPostgresqlBgwriterDurationDataPoint adds a data point to postgresql.bgwriter.duration metric.
func (mb *MetricsBuilder) RecordPostgresqlBgwriterDurationDataPoint(ts pcommon.Timestamp, val int64, bgDurationTypeAttributeValue AttributeBgDurationType) {
	mb.metricPostgresqlBgwriterDuration.recordDataPoint(mb.startTime, ts, val, bgDurationTypeAttributeValue.String())
}

// RecordPostgresqlBgwriterMaxwrittenCountDataPoint adds a data point to postgresql.bgwriter.maxwritten.count metric.
func (mb *MetricsBuilder) RecordPostgresqlBgwriterMaxwrittenCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricPostgresqlBgwriterMaxwrittenCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordPostgresqlBlockCountDataPoint adds a data point to postgresql.block.count metric.
func (mb *MetricsBuilder) RecordPostgresqlBlockCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricPostgresqlBlockCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordPostgresqlBlocksReadDataPoint adds a data point to postgresql.blocks_read metric.
func (mb *MetricsBuilder) RecordPostgresqlBlocksReadDataPoint(ts pcommon.Timestamp, val int64, databaseAttributeValue string, tableAttributeValue string, sourceAttributeValue AttributeSource) {
	mb.metricPostgresqlBlocksRead.recordDataPoint(mb.startTime, ts, val, databaseAttributeValue, tableAttributeValue, sourceAttributeValue.String())
}

// RecordPostgresqlCommitsDataPoint adds a data point to postgresql.commits metric.
func (mb *MetricsBuilder) RecordPostgresqlCommitsDataPoint(ts pcommon.Timestamp, val int64, databaseAttributeValue string) {
	mb.metricPostgresqlCommits.recordDataPoint(mb.startTime, ts, val, databaseAttributeValue)
}

// RecordPostgresqlDatabaseTableIndexScansDataPoint adds a data point to postgresql.database.table.index.scans metric.
func (mb *MetricsBuilder) RecordPostgresqlDatabaseTableIndexScansDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricPostgresqlDatabaseTableIndexScans.recordDataPoint(mb.startTime, ts, val)
}

// RecordPostgresqlDatabaseTableIndexSizeDataPoint adds a data point to postgresql.database.table.index.size metric.
func (mb *MetricsBuilder) RecordPostgresqlDatabaseTableIndexSizeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricPostgresqlDatabaseTableIndexSize.recordDataPoint(mb.startTime, ts, val)
}

// RecordPostgresqlDatabaseTransactionsDataPoint adds a data point to postgresql.database.transactions metric.
func (mb *MetricsBuilder) RecordPostgresqlDatabaseTransactionsDataPoint(ts pcommon.Timestamp, val int64, transactionResultAttributeValue AttributeTransactionResult) {
	mb.metricPostgresqlDatabaseTransactions.recordDataPoint(mb.startTime, ts, val, transactionResultAttributeValue.String())
}

// RecordPostgresqlDbSizeDataPoint adds a data point to postgresql.db_size metric.
func (mb *MetricsBuilder) RecordPostgresqlDbSizeDataPoint(ts pcommon.Timestamp, val int64, databaseAttributeValue string) {
	mb.metricPostgresqlDbSize.recordDataPoint(mb.startTime, ts, val, databaseAttributeValue)
}

// RecordPostgresqlOperationsDataPoint adds a data point to postgresql.operations metric.
func (mb *MetricsBuilder) RecordPostgresqlOperationsDataPoint(ts pcommon.Timestamp, val int64, databaseAttributeValue string, tableAttributeValue string, operationAttributeValue AttributeOperation) {
	mb.metricPostgresqlOperations.recordDataPoint(mb.startTime, ts, val, databaseAttributeValue, tableAttributeValue, operationAttributeValue.String())
}

// RecordPostgresqlQueryBlockCountDataPoint adds a data point to postgresql.query.block.count metric.
func (mb *MetricsBuilder) RecordPostgresqlQueryBlockCountDataPoint(ts pcommon.Timestamp, val int64, queryBlockOperationAttributeValue AttributeQueryBlockOperation, queryBlockTypeAttributeValue AttributeQueryBlockType) {
	mb.metricPostgresqlQueryBlockCount.recordDataPoint(mb.startTime, ts, val, queryBlockOperationAttributeValue.String(), queryBlockTypeAttributeValue.String())
}

// RecordPostgresqlQueryCountDataPoint adds a data point to postgresql.query.count metric.
func (mb *MetricsBuilder) RecordPostgresqlQueryCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricPostgresqlQueryCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordPostgresqlQueryDurationAverageDataPoint adds a data point to postgresql.query.duration.average metric.
func (mb *MetricsBuilder) RecordPostgresqlQueryDurationAverageDataPoint(ts pcommon.Timestamp, val float64) {
	mb.metricPostgresqlQueryDurationAverage.recordDataPoint(mb.startTime, ts, val)
}

// RecordPostgresqlQueryDurationTotalDataPoint adds a data point to postgresql.query.duration.total metric.
func (mb *MetricsBuilder) RecordPostgresqlQueryDurationTotalDataPoint(ts pcommon.Timestamp, val float64) {
	mb.metricPostgresqlQueryDurationTotal.recordDataPoint(mb.startTime, ts, val)
}

// RecordPostgresqlReplicationDataDelayDataPoint adds a data point to postgresql.replication.data_delay metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationDataDelayDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricPostgresqlReplicationDataDelay.recordDataPoint(mb.startTime, ts, val)
}

// RecordPostgresqlReplicationDelayDataPoint adds a data point to postgresql.replication.delay metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationDelayDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricPostgresqlReplicationDelay.recordDataPoint(mb.startTime, ts, val)
}

// RecordPostgresqlRollbacksDataPoint adds a data point to postgresql.rollbacks metric.
func (mb *MetricsBuilder) RecordPostgresqlRollbacksDataPoint(ts pcommon.Timestamp, val int64, databaseAttributeValue string) {
	mb.metricPostgresqlRollbacks.recordDataPoint(mb.startTime, ts, val, databaseAttributeValue)
}

// RecordPostgresqlRowsDataPoint adds a data point to postgresql.rows metric.
func (mb *MetricsBuilder) RecordPostgresqlRowsDataPoint(ts pcommon.Timestamp, val int64, stateAttributeValue AttributeState, databaseAttributeValue string, tableAttributeValue string) {
	mb.metricPostgresqlRows.recordDataPoint(mb.startTime, ts, val, stateAttributeValue.String(), databaseAttributeValue, tableAttributeValue)
}

// RecordPostgresqlTableCountDataPoint adds a data point to postgresql.table.count metric.
func (mb *MetricsBuilder) RecordPostgresqlTableCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricPostgresqlTableCount.recordDataPoint(mb.startTime, ts, val)
}

// Reset resets metrics builder to its initial state. It should be used when external metrics source is restarted,
// and metrics builder should update its startTime and reset it's internal state accordingly.
func (mb *MetricsBuilder) Reset(options ...metricBuilderOption) {
	mb.startTime = pcommon.NewTimestampFromTime(time.Now())
	for _, op := range options {
		op(mb)
	}
}
